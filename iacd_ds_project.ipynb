{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - EDA and models comparison\n",
    "\n",
    "*Autores: David Tejero Ruiz & Miguel Gil Castilla*\n",
    "\n",
    "El [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) ha sido obtenido de la página web de Kaggle en el apartado de competiciones. Este ha sido escogido debido a que está recomendado como un dataset de \"juguete\" con el que continuar el aprendizaje en python y ciencia del dato a un nivel más o menos básico. Nuestra idea es realizar un exploratory data análisis aplicando técnicas vistas en clase y ampliando estas con algunas ideas captadas de la red, tras esto trataremos nuestro dataset ya modificado para crear un modelo de regresión con el que predecir la variable respuesta *PriceSale*.\n",
    "\n",
    "Una particularidad de este dataset es su elevado número de atributos (79), lo que lo hace muy adecuado para poner en práctica algunas técnicas vistas de ingeniería de características. Así, podemos extraer información relevante de ellas para realizar una predicción más o menos precisa del precio de venta de las diferentes viviendas.\n",
    "\n",
    "\n",
    "Dado que en el conjunto test.csv no se proporciona el valor de la variable respuesta (era objetivo obtenerlo para la competición de Kaggle al que pertenece), nos hemos centrado en el conjunto *train.csv*, que hemos renombrado al archivo *data.csv* que se encuentra en este directorio, y será el conjunto de datos que analicemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Importamos el conjunto de datos\n",
    "import pandas as pd\n",
    "House_prices = pd.read_csv('data.csv')\n",
    "\n",
    "# Para evitar avisos innecesarios:\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que Pandas por defecto, trata los valores NA (Not Available) como NaN (Not a Number). Si se analiza la descripción de los datos, NA es un valor que pueden tomar algunos atributos categóricos (no todos), por lo que no se trata de un valor perdido como tal, sino que  indica que no se puede calcular el valor del atributo, porque la vivienda no dispone de el item que se está evaluando. Por ejemplo, en el caso del atributo de calidad de la piscina, NA indica que la vivienda no dispone de piscina, lo cuál es información relevante, y no un valor perdido que tendríamos que imputar.\n",
    "\n",
    "Hemos analizado la descripción de los diferentes atributos categóricos y aquellos en los que un NA podría aportar información relevante, son:\n",
    "\n",
    "- Alley\n",
    "\n",
    "- BsmtQual\n",
    "\n",
    "- BsmtCond\n",
    "\n",
    "- BsmtExposure\n",
    "\n",
    "- BsmtFinType1\n",
    "\n",
    "- BsmtFinType2\n",
    "\n",
    "- FireplaceQu\n",
    "\n",
    "- GarageType\n",
    "\n",
    "- GarageYrBlt\n",
    "\n",
    "- GarageFinish\n",
    "\n",
    "- GarageQual\n",
    "\n",
    "- GarageCond\n",
    "\n",
    "- PoolQC\n",
    "\n",
    "- Fence\n",
    "\n",
    "- MiscFeature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_attributes = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                 'PoolQC', 'Fence', 'MiscFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la cabecera de los datos\n",
    "House_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el dataset está compuesto por 81 columnas, de las cuales 79 serán las características que usaremos en la predicción, y las dos restantes sol el id (que no aporta información), y la variable que queremos predecir (precio de venta de la casa).\n",
    "\n",
    "Además notamos que existe una riqueza de características alta, encontrando tanto valores numéricos como categóricos, cosa que analizaremos a continuación en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a definir un problema de regresión sobre el precio de las casas, por lo que la variable \n",
    "# objetivo del dataset será 'SalePrice' (última columna de nuestro dataset como es habitual)\n",
    "y_data = House_prices['SalePrice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de missing values y non-available values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentamos al inicio, el dataset contiene valores NA, y en algunos casos realmente podrían aportar información útil al decir que no se tiene el objeto que tratamos de evaluar en el atributo. Vamos a ver qué atributos contienen valores NA, y cuántos de estos valores NA contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a contar las apariciones de NaNs en nuestro dataset al completo\n",
    "print(\"Hay \", House_prices.isnull().sum().sum(), \" valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el conjunto de datos en atributos con posibles valores NA y \n",
    "# atributos con posibles missing values\n",
    "House_prices_na = House_prices[na_attributes]\n",
    "House_prices_mv = House_prices.drop(na_attributes, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuales de los atributos en los que un NaN significa Missing Value \n",
    "# tienen realmente valores NaN\n",
    "mv_counts = House_prices_mv.isnull().sum()\n",
    "mv_counts = mv_counts[mv_counts > 0]\n",
    "print(\"Atributos que contienen missing values: \\n{}\".format(mv_counts))\n",
    "print(\"Hay \", len(mv_counts), \" atributos que contienen missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De entre aquellos en los que NA no es un valor posible del atributo,\n",
    "# y las apariciones de NaN son datos pérdidos, vamos a ver cuáles son \n",
    "# numéricos y cuáles no\n",
    "House_prices_mv_numerics = House_prices_mv[mv_counts.keys()].select_dtypes(include=[np.number])\n",
    "House_prices_mv_categorics = House_prices_mv[mv_counts.keys()].select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen missing values: \\n{}\".format(House_prices_mv_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen missing values: \\n{}\".format(House_prices_mv_categorics.keys().to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos numéricos que contienen missing values los vamos a rellenar con la media\n",
    "# de los valores de la columna\n",
    "House_prices_mv_numerics = House_prices_mv_numerics.fillna(House_prices_mv_numerics.mean())\n",
    "\n",
    "# Los atributos categóricos que contienen missing values los vamos a rellenar con el valor\n",
    "# más frecuente de la columna\n",
    "House_prices_mv_categorics = House_prices_mv_categorics.fillna(House_prices_mv_categorics.mode().iloc[0])\n",
    "\n",
    "# Unimos los dos conjuntos de datos\n",
    "House_prices_mv_fixed = pd.concat([House_prices_mv_numerics, House_prices_mv_categorics], axis=1)\n",
    "\n",
    "# Guardamos los datos tratados\n",
    "for key in mv_counts.keys():\n",
    "    House_prices[key] = House_prices_mv_fixed[key]\n",
    "\n",
    "# Vamos a comprobar que ya no hay valores nulos en los atributos que hemos tratado\n",
    "if (House_prices[mv_counts.keys()].isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Available values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que atributos contienen valores NA entre los que NA aporta información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuales de los atributos en los que un NaN significa Missing Value \n",
    "# tienen realmente valoress NaN\n",
    "na_counts = House_prices_na.isnull().sum()\n",
    "na_counts = na_counts[na_counts > 0]\n",
    "print(\"Atributos que contienen Non Available values: \\n{}\".format(na_counts))\n",
    "print(\"Hay \", len(na_counts), \" atributos que contienen Non Available values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na.select_dtypes(include=[np.number])\n",
    "House_prices_na_categorics = House_prices_na.select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen Non Available values: \\n{}\".format(House_prices_na_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen Non Available values: \\n{}\".format(House_prices_na_categorics.keys().to_list()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar tratando el caso especial del único atributo numérico que tiene un NA como valor informativo, hablamos del **GarageYrBlt**. Este atributo codifica el año en el que se construyó el garaje de la casa, y un valor NA indica que la casa no tiene garaje. La información de \"no tenemos garaje\" puede ser relevante, por lo que tenemos que tratar este caso cuidosamente.\n",
    "\n",
    "Sin embargo, parte de los atributos que analizaremos un poco más adelante: GarageType, GarageFinish, GarageCond y Garageal, son atributos categóricos cuyo valor NA indica igualmente \"no tenemos garaje\", por lo que esa información en este atributo en particular es redundante.\n",
    "\n",
    "Por tanto, hemos llegado a la decisión de sustituir los valores NA de GarageYrBlt por la media de los valores no NA de este atributo. De esta forma, cuando posteriormente le apliquemos una normalización estándar al atributo y le restemos la media, obtendremos un valor nulo, y por tanto no afectará a la predicción dado a que así indicaremos que no es relevante ese valor en este atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na_numerics.fillna(House_prices_na_numerics.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el resto de valores NA, los NaNs indican un posible valor categorico, por lo que vamos a sustituirlos por la cadena \"None\", para que se traten como un posible valor categórico más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos los NaNs con la cadena 'None\n",
    "House_prices_na_categorics = House_prices_na_categorics.fillna('None')\n",
    "\n",
    "# Unimos los dos conjuntos de datos tratados\n",
    "House_prices_na_fixed = pd.concat([House_prices_na_numerics, House_prices_na_categorics], axis=1)\n",
    "\n",
    "# Guardamos los datos tratados\n",
    "for key in na_counts.keys():\n",
    "    House_prices[key] = House_prices_na_fixed[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a comprobar que ya no hay valores nulos en nuestro conjunto de datos\n",
    "if (House_prices.isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase inicial de ingeniería de características\n",
    "Vamos a analizar estas sustituticiones con los valores 'None' que repercusión tiene sobre el conjunto de datos. Se mostrará una gráfica con la cantidad de estos valores para valorar si es necesario seguir con la exploración o simplemente eliminar las estancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a graficar los valores NA que susituimos por 'None'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ordenados_na = na_counts.sort_values(ascending=False)\n",
    "plt.bar(ordenados_na.index, ordenados_na)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Número de valores NA')\n",
    "plt.xlabel('Atributos')\n",
    "plt.title('Valores NA sustituidos por None')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print de los 4 mayores:\n",
    "print(\"Los 4 mayores valores NA sustituidos por None son:\")\n",
    "print(ordenados_na.head(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que existe una cantidad exuberante de datos marcados con None, superando el 50% de las instancias, para 4 de los atributos, concretamente:\n",
    " - PoolQC: 1453\n",
    " - MiscFeature: 1406 \n",
    " - Alley: 1369\n",
    " - Fence: 1179\n",
    " \n",
    "Debemos de considerar que el número total de instancias en el dataset es de **1460**.\n",
    "\n",
    "Para ello dibujaremos el par atributo-variable respuesta para ver la relación entre estos dos, concretamente en los valores que no son None. Con esta comparación veremos si es relevante la información que nos aporta o no el atributo, decidiendo si eliminarlo o no.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos histograma de la variable objetivo, para ver su distribución\n",
    "plt.hist(y_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da la intuición de dónde se sitúa la media de la variable respuesta y somo es su distribución, con ella podremos decidir si eliminar o no algunso atributos. Nuestra idea es que si los valores que no son 'None' son pocos y se sitúan cercanos a la donde de encuentran la mayoría de instancias (cercanos a la media de la variable respuesta), probablemente estos atributos no aportarán información relevante para la regresión, comparado con la información de otros atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los valores  de las 4 variables con mayor número de valores NA\n",
    "# en aquellos que son diferentes de None.\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "ax[0,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[0]])\n",
    "ax[0,0].set_xlabel('SalePrice')\n",
    "ax[0,0].set_ylabel(ordenados_na.index[0])\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[1]])\n",
    "ax[0,1].set_xlabel('SalePrice')\n",
    "ax[0,1].set_ylabel(ordenados_na.index[1])\n",
    "ax[0,1].grid()\n",
    "\n",
    "ax[1,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[2]])\n",
    "ax[1,0].set_xlabel('SalePrice')\n",
    "ax[1,0].set_ylabel(ordenados_na.index[2])\n",
    "ax[1,0].grid()\n",
    "\n",
    "ax[1,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[3]])\n",
    "ax[1,1].set_xlabel('SalePrice')\n",
    "ax[1,1].set_ylabel(ordenados_na.index[3])\n",
    "ax[1,1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en las gráficas comparativas, se observa que los atributos toman 'None' a lo largo de todo el rango de valores de SalePrice. Vemos que los valores que no son None de los atributos categóricos son pocos y se encuentran además en la zona donde tenemos un alto número de instancias, con lo cual, no aportan información relevante en este problema de regresión. Es por esto por lo que decidimos directamente eliminar estos atributos categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los 4 atributos que más NA contienen \n",
    "House_prices = House_prices.drop(ordenados_na.head(4).index, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de nuestro dataset\n",
    "Tras haber realizado un primer análisis inicial, vamos a construir el dataset de partida, que será el que usaremos para realizar el análisis exploratorio de datos y la construcción de los modelos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el dataset de datos de entrada (X_data) como el dataset sin la variable objetivo\n",
    "# y sin la variable 'Id' (que no aporta información)\n",
    "X_data = House_prices.drop(['SalePrice', 'Id'], axis=1)\n",
    "\n",
    "# Para tratar los diferentes atributos lo primero es distinguir entre las variables cuantitativas \n",
    "# (numéricas) y cualitativas (categóricas)\n",
    "# Ya que el tipo de dato 'object' engloba a las variables categóricas y a las variables de tipo\n",
    "# # string, se puede usar dicho tipo de dato para distinguir las variables en numéricas y categóricas\n",
    "numerical_atr = [col for col in X_data.columns if X_data.dtypes[col] != 'object']\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "\n",
    "print(\"Hay \", len(numerical_atr), \" datos numéricos: \",numerical_atr)\n",
    "print(\"Hay \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las variables categoricas\n",
    "X_data[categorical_atr].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como bien sabemos, estos atributos categóricos debemos transformarlos a valores numéricos para poder trabajar con ellos. Existen diferentes técnicas para realizar esto, y para hacerlo de forma consistente, debemos analizar la naturaleza de los diferentes atributos categóricos.\n",
    "\n",
    "Notamos que existen una serie de variables categóricas a cuyos atributos podemos asignarles un valor numérico según un orden. Estos casos son cuando hablamos de calidades (a mayor calidad vamos a asignar un valor superior) o cuando cuantificamos con palabras una cantidad (por ejemplo, el LandSlope caracteriza la pendiente del terreno, y podemos asignarle un valor numérico mayor a una mayor pendiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basándonos en el archivo data_description.txt codificamos las variables categóricas con información cuantitativa\n",
    "X_data['LotShape'] = X_data['LotShape'].map({'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0})\n",
    "X_data['LandSlope'] = X_data['LandSlope'].map({'Gtl': 0, 'Mod': 1, 'Sev': 2})\n",
    "X_data['ExterQual'] = X_data['ExterQual'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['ExterCond'] = X_data['ExterCond'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['BsmtQual'] = X_data['BsmtQual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['BsmtCond'] = X_data['BsmtCond'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['BsmtExposure'] = X_data['BsmtExposure'].map({'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0})\n",
    "X_data['BsmtFinType1'] = X_data['BsmtFinType1'].map({'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['BsmtFinType2'] = X_data['BsmtFinType2'].map({'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['HeatingQC'] = X_data['HeatingQC'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['KitchenQual'] = X_data['KitchenQual'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['Functional'] = X_data['Functional'].map({'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0})\n",
    "X_data['FireplaceQu'] = X_data['FireplaceQu'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['GarageFinish'] = X_data['GarageFinish'].map({'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['GarageQual'] = X_data['GarageQual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['GarageCond'] = X_data['GarageCond'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['PavedDrive'] = X_data['PavedDrive'].map({'Y': 2, 'P': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas dos características podemos tratarlas como binarias al tener solo dos posibles valores\n",
    "X_data['CentralAir'] = X_data['CentralAir'].map({'Y': 1, 'N': 0})\n",
    "X_data['Street'] = X_data['Street'].map({'Pave': 1, 'Grvl': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las que nos quedan, son variables categóricas que no podemos ordenar, y por tanto, debemos aplicar una codificación one-hot. Para ello, vamos a usar la función OneHotEncoder de sklearn, que nos permite realizar esta codificación de forma sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculamos los valores que siguen siendo categóricos\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "print(\"Quedan \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Usamos OneHotEncoder para codificar las variables categóricas\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_data_encoded = pd.DataFrame(encoder.fit_transform(X_data[categorical_atr]))\n",
    "X_data_encoded.columns = encoder.get_feature_names(categorical_atr)\n",
    "\n",
    "# Borramos las variables categóricas originales y añadimos las nuevas codificadas\n",
    "X_data = X_data.drop(categorical_atr ,axis=1)\n",
    "X_data = pd.concat([X_data, X_data_encoded], axis=1)\n",
    "\n",
    "# Mostramos los datos de entrada\n",
    "X_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencionar aunque 214 parezcan muchos atributos, gracias al label encoding manual que hemos realizado arriba usando información de la descripción de los datos, nos hemos ahorrado hacer one-hot-encoding de todos los atributos categóricos, lo cual supondría obtener bastantes más atributos, aumentando consideralmente la complejidad del problema.\n",
    "\n",
    "Notar que hemos optado por usar one-hot-encoding para las variables en las que no podemos definir un orden, y no otro tipo de codificaciones, para no introducir un orden artificial que no existiría en la realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos que no hay valores nulos en el conjunto final de datos\n",
    "if (X_data.isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")\n",
    "\n",
    "# Probamos que no quedan variables categóricas\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "print(\"Quedan \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingeniería de características avanzada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenermos la matriz de correlación de los datos de entrada\n",
    "correlation_matrix = X_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de correlación de Pearson entre los atributos\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "sns.heatmap(correlation_matrix, cmap='viridis',vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a identificar pares de atributos con correlación mayor a 0.7\n",
    "# y vamos a eliminar uno de los dos atributos de cada par\n",
    "# para evitar la multicolinealidad\n",
    "\n",
    "# Mantenemos una lista de atributos eliminados para no eliminarlos dos veces, y simplifcamos\n",
    "\n",
    "# La matriz es simétrica, por lo que definimos los bucles para recorrer solo la mitad superior\n",
    "# de la matriz\n",
    "\n",
    "# Tenemos en cuenta tanto correlación positiva como negativa, por lo que usamos el valor absoluto\n",
    "\n",
    "umbral = 0.7\n",
    "atributos_eliminados = []\n",
    "for i in range(len(correlation_matrix.columns)): \n",
    "    for j in range(i+1, len(correlation_matrix.columns)): \n",
    "        if (np.abs(correlation_matrix.iloc[i, j]) >= umbral):\n",
    "            if (correlation_matrix.columns[j] not in atributos_eliminados):\n",
    "                atributo = correlation_matrix.columns[j]\n",
    "                atributos_eliminados.append(atributo)\n",
    "                print(\"Eliminamos el atributo \", atributo, \" por tener una correlación mayor a \", umbral, \" con el atributo \", correlation_matrix.columns[i])\n",
    "\n",
    "print(\"Se van a eliminar \", len(atributos_eliminados), \" atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los atributos de la lista de atributos a eliminar\n",
    "X_data = X_data.drop(atributos_eliminados, axis=1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metodos de filtrado para selección de características\n",
    "A continuación, vamos a estudiar la correlación de los atributos con la variable respuesta, para ver si podemos eliminar algunos atributos que no aporten información relevante para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.DataFrame(pd.concat([X_data, y_data], axis=1)).corr()\n",
    "correlation_matrix = correlation_matrix['SalePrice']\n",
    "\n",
    "# Ploteamos la matriz de correlación de los datos de entrada\n",
    "plt.figure(figsize=(15, 0.5))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "sns.heatmap(correlation_matrix.to_frame().T, cmap='viridis', vmax=1, vmin=-1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a eliminar los atributos que tengan una correlación pequeña con la variable objetivo\n",
    "umbral = 0.025\n",
    "atributos_a_eliminar = []\n",
    "for i in range(len(correlation_matrix)):\n",
    "    if (np.abs(correlation_matrix[i]) < umbral):\n",
    "        atributo = correlation_matrix.index[i]\n",
    "        atributos_a_eliminar.append(atributo)\n",
    "        print(\"Eliminamos el atributo \", atributo, \" por tener una correlación menor a \", umbral, \" con la variable objetivo\")\n",
    "\n",
    "print(\"Se van a eliminar \", len(atributos_a_eliminar), \" atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los atributos de la lista de atributos a eliminar\n",
    "X_data = X_data.drop(atributos_a_eliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Outliers\n",
    "Vamos a añadir un análisis de outliers, para ver si podemos eliminar algunas intancias que no aporten información relevante para la predicción.\n",
    "\n",
    "El proceso seguido en el código que mostramos a continuación para detectar y eliminar outliers, se resume en los siguientes pasos:\n",
    "\n",
    "1. **Cálculo de los límites de los outliers**: para cada atributo en el conjunto de datos, calculamos el primer cuartil (q1), el tercer cuartil (q3) y el rango intercuartílico (IQR) (diferencia entre q3 y q1). Con ellos determinaremos los umbrales inferior y superior de los outliers. Generalmente, se suele ser coger un umbral superior a 1.5\\*IQR por encima del q3, y un umbral inferior a 1.5\\*IQR por debajo del q1. \n",
    "\n",
    "2. **Identificación de outliers por atributo**: Cualquier valor fuera de los umbrales, será considerado un outlier, por lo que se utiliza la condición (X_data[key] < umbral_inferior) | (X_data[key] > umbral_superior) para encontrar los valores que son outliers en cada atributo. Estos valores se guardan en un diccionario donde la clave es el nombre del atributo y el valor es un índice de las instancias que son outliers en ese atributo.\n",
    "\n",
    "3. **Conteo de ocurrencias de instancias**: Se crea otro nuevo diccionario para contar el número de ocurrencias que cada instancia es considerada outlier por diferentes atributos.\n",
    "\n",
    "4. **Identificación de outliers en un número mínimo de atributos**: Se define un umbral mínimo, que determina cuántos atributos deben considerar una instancia como outlier para que se elimine. Aquellas instancias que superen este umbral, serán consideradas finalmente outliers.\n",
    "\n",
    "5. **Eliminación de outliers**: Finalmente, se eliminan las instancias identificadas como outliers del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificamos potenciales outliers por cada atributo\n",
    "outliers = {}\n",
    "for key in X_data.keys():\n",
    "    q1 = X_data[key].quantile(0.25)\n",
    "    q3 = X_data[key].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    umbral_inferior = q1 - 1.5 * iqr\n",
    "    umbral_superior = q3 + 1.5 * iqr\n",
    "    outliers_encontrados = X_data[(X_data[key] < umbral_inferior) | (X_data[key] > umbral_superior)].index\n",
    "    outliers[key] = outliers_encontrados\n",
    "\n",
    "# Identificamos las instancias que son outliers en un número mínimo de atributos\n",
    "minimo_atributos_para_eliminar = 10\n",
    "indices_outliers = []\n",
    "contador_instancias = {}\n",
    "\n",
    "# Contamos el número de ocurrencias de cada instancia como outlier\n",
    "for atributo, indices in outliers.items():\n",
    "    for indice in indices:\n",
    "        if indice not in contador_instancias:\n",
    "            contador_instancias[indice] = 1\n",
    "        else:\n",
    "            contador_instancias[indice] += 1\n",
    "\n",
    "# Identificamos los índices de las instancias que son outliers en un número mínimo de atributos\n",
    "for instancia, contador in contador_instancias.items():\n",
    "    if contador >= minimo_atributos_para_eliminar:\n",
    "        indices_outliers.append(instancia)\n",
    "\n",
    "# Eliminamos los outliers\n",
    "X_data = X_data.drop(indices_outliers, axis=0)\n",
    "y_data = y_data.drop(indices_outliers, axis=0)\n",
    "\n",
    "print(\"Se han eliminado \", len(indices_outliers), \" instancias\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el conjunto de datos en tres subconjuntos: entrenamiento, validación y test\n",
    "# En una proprorción 50% - 20% - 30% respectivamente\n",
    "\n",
    "train_size = 0.7\n",
    "val_size = 0.2\n",
    "test_size = 0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Realizamos esta división en train y test para hacer validación cruzada con un conjunto de entrenamiento que será dividido en folds y un conjunto de test \n",
    "# que será usado para evaluar el modelo final como datos nunca vistos\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(X_data, y_data, test_size=(1-train_size), random_state=41)\n",
    "# Dividimos el conjunto auxiliar en validación y test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=test_size/(test_size + val_size), random_state=41)\n",
    "\n",
    "# Mostramos los tamaños de los conjuntos de datos\n",
    "print(\"Tamaño del conjunto de entrenamiento: \", len(X_train))\n",
    "print(\"Tamaño del conjunto de test: \", len(X_test))\n",
    "print(\"Tamaño del conjunto de validación: \", len(X_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las variables numéricas para que todas tengan media 0 y desviación típica 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_data.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_data.columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_data.columns)\n",
    "\n",
    "# Mostramos los datos de entrada\n",
    "X_train.head()\n",
    "\n",
    "X_data = pd.concat([X_train, X_val, X_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos\n",
    "Los modelos escogidos para realizar el entrenamiento y que más tarde pondremos a prueba serán:\n",
    "- Regresión Lineal\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "\n",
    "## Proceso de elección del modelo de regresión\n",
    "Para escoger el modelo de regresión, vamos a realizar los siguientes pasos:\n",
    "1. Dividimos los datos en **tres** conjuntos: entrenamiento, validación y test.\n",
    "2. Escogemos un modelo de regresión entre los que hemos mencionado anteriormente.\n",
    "3. Entrenamos dicho modelo con los datos de entrenamiento y realizamos un **ajuste de sus hiperparámetros** con los datos de validación.\n",
    "4. Finalmente, para el conjunto con \"mejores\" hiperparámetros obtenidos, realizaremos **validación cruzada** uniendo los conjuntos de entrenamiento y validación, obteniendo una medida robusta de las métricas de rendimiento del modelo.\n",
    "5. Repetimos estos pasos anteriores para cada uno de los modelos de regresión que queremos comparar.\n",
    "6. Escogemos el modelo que mejores métricas tenga en la validación cruzada, lo entrenamos con todo el conjunto de datos de entrenamiento y validación, y lo **evaluamos en el conjunto de test**, que no hemos usado hasta ahora, para dar así una medida de rendimiento realista del modelo final.\n",
    "\n",
    "\n",
    "## Métricas\n",
    "Las métricas de evaluación del rendimiento del modelo que elegimos son las siguientes:\n",
    "\n",
    "- Error absoluto medio (MAE): Esta métrica se define como el error medio, es decir, variable respuesta - valor predicho : $MAE = \\frac{\\sum|y-\\hat y|}{N}$ (donde N es el número de instancias). Este nos da el error en la misma unidad que la variable respuesta, con lo cual es bastante intuitivo. En cambio, no es derivable con lo que habría que usar métodos como el descenso por el gradiente como optimizador para el entrenamiento del modelo. \n",
    "\n",
    "- Error cuadrático medio (MSE): Esta métrica se define como: $MSE = \\frac{\\sum(y-\\hat y)²}{N}$. Este fue el que utilizamos en una primera instancia y confundió nuestro entendimiento de los resultados al tener valores tan elevados. Esto es debido a que evidentemente es por su valor cuadrático, aún así es interesante porque puede usarse como función de pérdida.\n",
    "\n",
    "- La métrica R2: también llamada coeficiente de determinación, es una métrica que indica qué tan bien se ajusta un modelo de regresión a los datos observados, variando entre 0 y 1. Un valor cercano a 1 indica un buen ajuste, mientras que un valor cercano a 0 significa que el modelo no puede explicar la variabilidad de los datos. De hecho, un valor de cero, sería lo que obtendría un modelo que devuelva siempre el valor medio de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las métricas que emplearemos para evaluar el rendimiento de los modelos\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que nuestra cantidad de datos no es muy grande, vamos a usar validación cruzada en lugar del método holdout para devolver una medida de rendimiento final:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividimos el conjunto de entrenamiento en 5 subconjuntos\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Creamos una función para hacer la validación cruzada\n",
    "def cross_validation(model, X, y, n_splits=10):\n",
    "\n",
    "    # Aplicamos  shuffle para que los datos se mezclen antes de dividirlos\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    splits = kf.split(X)\n",
    "    \n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    scores_r2 = []\n",
    "\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        # Extraemos el conjunto de entrenamiento y el conjunto de validación indexando\n",
    "        # con los índices de cada conjunto aportados por la función .split()\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Entrenamos el modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluamos el modelo\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        scores_mae.append(mean_absolute_error(y_pred, y_val))\n",
    "        scores_mse.append(mean_squared_error(y_pred, y_val))\n",
    "        scores_r2.append(r2_score(y_pred, y_val))\n",
    "   \n",
    "    return np.mean(scores_mse), np.mean(scores_mae), np.mean(scores_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra utilidad que vamos a necesitar es una función para evaluar los modelos para diferentes hiperparámetros. La idea es crear una función que reciba unos conjuntos de entrenamiento y validadación, un diccionario con los hiperparámetros del modelo, y una métrica de evaluación.\n",
    "\n",
    "Tendremos que crear todas las posibles combinaciones de valores de hiperparámetros que queremos evaluar en función al diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos un módulo que nos ayudará a realizar las iteraciones por cada tipo de\n",
    "# hiperparámetro y sus valores de manera eficiente y sin tener que usar los for \n",
    "# de python no optimizados\n",
    "\n",
    "import itertools\n",
    "\n",
    "def hyperparams_evaluation(Model, Xtrain, ytrain, Xval, yval, hyperparams_dict, metric_function=mean_squared_error, minimize=True, show_iteratioins=False):\n",
    "    \n",
    "    # Lista de valores de hiperparámetros\n",
    "    # Es decir, esto devolverá una lista de listas como la que vemos a continuación:\n",
    "    # [[hyperparam1_val1, hyperparam1_val2,...], [hyperparam2_val1, hyperparam2_val2,...],...]\n",
    "    hyperparams_values = list(hyperparams_dict.values())\n",
    "\n",
    "    # Lista de nombres de hiperparámetros\n",
    "    # Es decir, esto devolverá una lista como la que vemos a continuación:\n",
    "    # [hyperparam1_name, hyperparam2_name,...]\n",
    "    hyperparams_keys = list(hyperparams_dict.keys())\n",
    "\n",
    "    # Creamos una lista con todas las combinaciones de valores de hiperparámetros\n",
    "    # Es decir, esto devolverá una lista de tuplas como la que vemos a continuación:\n",
    "    # [(hyperparam1_val1, hyperparam2_val1,...), (hyperparam1_val1, hyperparam2_val2,...),...]\n",
    "    # Para ello usamos la función product del módulo itertools\n",
    "    combinations = list(itertools.product(*hyperparams_values))\n",
    "    \n",
    "    # Lista para almacenar los resultados de la evaluación\n",
    "    results = []\n",
    "\n",
    "    for values in combinations:\n",
    "        \n",
    "        # Creamos un diccionario con los valores de los hiperparámetros de esta iteración\n",
    "        hyperparams = dict(zip(hyperparams_keys, values))\n",
    "\n",
    "        # Creamos un modelo con ellos\n",
    "        model = Model(**hyperparams)\n",
    "\n",
    "        # Entrenamos el modelo en entrenamiento\n",
    "        model.fit(Xtrain, ytrain)\n",
    "        \n",
    "        # Evaluamos el modelo en validación\n",
    "        y_pred = model.predict(Xval)\n",
    "        \n",
    "        # Aplicamos la métrica de evaluación y almacenamos el resultado\n",
    "        results.append(metric_function(y_pred, yval))\n",
    "\n",
    "        if show_iteratioins:\n",
    "            print(\"Iteración: \", len(results), \" / \", len(combinations), \" - Resultado: \", results[-1])\n",
    "    \n",
    "    # Como la métrica es genérica, distinguimos si queremos maximizar o minimizar\n",
    "    if minimize:\n",
    "        best_idx = np.argmin(results)\n",
    "    else:\n",
    "        best_idx = np.argmax(results)\n",
    "    \n",
    "    print(\"Mejor resultado: \", results[best_idx])\n",
    "    print(\"Mejor combinación de hiperparámetros: \", dict(zip(hyperparams_keys, combinations[best_idx])))\n",
    "    \n",
    "    # Devolvemos la mejor combinación de hiperparámetros y los resultados de todas las iteraciones\n",
    "    best_hyperparams = dict(zip(hyperparams_keys, combinations[best_idx]))\n",
    "    return results, best_hyperparams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "En primer lugar comenzaremos con un modelo lo más simple posible para casos de regresión, el modelo visto en clase *Regresión Lineal*, así tendremos una base con la que comparar modelos más complejos a posteriori. Con esto conseguiremos algo así como un modelo *\"Dummy\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargamos el modelo de regresión lineal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creamos el modelo, entrenamos con el conjunto de entrenamiento \n",
    "# y validamos con el conjunto de validación\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred_val = reg.predict(X_val)\n",
    "\n",
    "# Calculamos las métricas de rendimiento\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos los resultados obtenidos no son nada buenos, obteniendo unos errores medios enormes, y un R2 muy negativo, vamos a analizar un poco qué es lo que está pasando. Lo primero de todo, vamos a ver que métricas obtiene el modelo de regresión lineal sobre el conjunto de entrenamiento, para ver si el modelo está sobreajustando o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas sobre el conjunto de entrenamiento\n",
    "y_pred_train = reg.predict(X_train)\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las métricas sobre el conjunto de train son mucho mejores, pasando , lo que nos indica que el modelo está posiblemente sobreajustado y no estamos generalizando bien. Para confirmar esto, vamos a ver qué valores predice el modelo sobre ambos conjuntos, y vamos a compararlos con los valores reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "ax[0].scatter(y_train, y_pred_train)\n",
    "ax[0].set_xlabel('y real (train)')\n",
    "ax[0].set_ylabel('y predicha (train)')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].scatter(y_val, y_pred_val)\n",
    "ax[1].set_xlabel('y real (validación)')\n",
    "ax[1].set_ylabel('y predicha (validación)')\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos existe mínimo una instancia que predice un valor desorbitado que no se corresponde con el valor real. Esta instancia está afectando mucho a las métricas, vamos a ver qué instancia es y qué valores tiene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a identificar la instancia del conjunto de validación con mayor error\n",
    "y_pred_val = pd.Series(y_pred_val, index=y_val.index)\n",
    "y_error = y_val - y_pred_val\n",
    "y_error = y_error.abs()\n",
    "y_error = y_error.sort_values(ascending=False)\n",
    "print(\"La instancia con mayor error es la \", y_error.index[0], \" con un error de \", y_error[y_error.index[0]])\n",
    "\n",
    "# Eliminamos esa instancia de los conjuntos y_val e y_pred_val\n",
    "y_val_fixed = y_val.drop(y_error.index[0])\n",
    "y_pred_val_fixed = y_pred_val.drop(y_error.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las métricas de rendimiento para el conjunto de validación sin la instancia con mayor error\n",
    "mae = mean_absolute_error(y_val_fixed, y_pred_val_fixed)\n",
    "mse = mean_squared_error(y_val_fixed, y_pred_val_fixed)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val_fixed, y_pred_val_fixed)\n",
    "\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la misma gráfica que antes pero sin la instancia con mayor error\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "ax[0].scatter(y_train, y_pred_train)\n",
    "ax[0].set_xlabel('y real (train)')\n",
    "ax[0].set_ylabel('y predicha (train)')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].scatter(y_val_fixed, y_pred_val_fixed)\n",
    "ax[1].set_xlabel('y real (validación)')\n",
    "ax[1].set_ylabel('y predicha (validación)')\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, las métricas pasan a obtener un valor mucho mejor, tanto el mae como el mse bajan considerablemente, y el R2 sube, tomando el valor de 0.91, un valor bastante aceptable y similar a las métricas obtenidas sobre el conjunto de entrenamiento. \n",
    "\n",
    "Esto nos indica que el modelo puede generalizar bien, pero hay alguna instancia que no es capaz de predecir, bien porque sea algún outlier que no hemos identificado, o bien porque el modelo no es capaz de generalizar bien en esa zona del espacio de características.\n",
    "\n",
    "Para tratar de explicar este resultado, vamos a ver los pesos del modelo de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficamos los coeficientes\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.bar(X_train.columns, reg.coef_)\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Coeficientes')\n",
    "plt.title('Coeficientes de la regresión lineal')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica anterior, se puede observar el motivo del sobreajuste sobre el conjunto de entrenamiento. Esto es provocado por el incremento no controlado de los pesos que se calculan en el aprendizaje, existe una desproporción en la cuantificación de estos y al multiplicar a los valores del conjunto de validación puede dar lugar a predicciones desmesuradas. Por ello, vimos necesario aplicar técnicas de regularización que nos permitieran decrementar el valor de estos coeficientes. Simplemente para confirmar nuestras ideas y descartar que las malas predicciones vienen dadas por outliers no detectados en el preprocesado de los datos. La regularización escogida es la denominada **Ridge**, pero antes de probarla, vamos a dar un rendimiento final del modelo básico de regresión lineal usando validación cruzada.\n",
    "\n",
    "Mencionar que el modelo básico de regresión lineal, no tiene hiperparámetros relevantes que ajustar, por lo que es no es necesario realizar una búsqueda de hiperparámetros, ya que al fin y al cabo, encontrará el hiperplano que reduzca el error cuadrático medio con los datos de entrenamiento, y el resultado final será el mismo para los mismos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos validación cruzada en el conjunto de entrenamiento más validación\n",
    "lr = LinearRegression()\n",
    "mse, mae, r2 = cross_validation(lr, pd.concat([X_train, X_val]), pd.concat([y_train, y_val]), n_splits=10)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"Validación cruzada:\")\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, los resultados no son muy buenos, tal y como esperábamos. Hemos empleado una partición de k = 10 para la validación cruzada, y obtenemos unas métricas no muy buenas. Esto es debido, a que el modelo no es capaz de ajustarse de forma correcta a instancias más \"límites\" que van cayendo de forma esporádica en el conjunto de validación tomado, haciendo que haya particiones para las que los errores sean muy grandes, y estropee la media final."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge en modelo de Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_val = ridge.predict(X_val)\n",
    "\n",
    "# Calculamos las métricas de rendimiento\n",
    "mae = mean_absolute_error(y_val, y_pred_val)\n",
    "mse = mean_squared_error(y_val, y_pred_val)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"--CONJUNTO DE VALIDACION--\")\n",
    "print(\" - MAE: \", mae)\n",
    "print(\" - MSE: \", mse)\n",
    "print(\" - RMSE: \", rmse)\n",
    "print(\" - R2: \", r2,\"\\n\")\n",
    "\n",
    "# Metricas sobre el conjunto de entrenamiento\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"--CONJUNTO DE ENTRENAMIENTO--\")\n",
    "print(\" - MAE: \", mae)\n",
    "print(\" - MSE: \", mse)\n",
    "print(\" - RMSE: \", rmse)\n",
    "print(\" - R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos a continuación los valores de los pesos para verificar que nuestras hipótesis eran ciertas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficamos los coeficientes\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.bar(X_train.columns, ridge.coef_)\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Coeficientes')\n",
    "plt.title('Coeficientes de la regresión lineal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, los pesos de los atributos son mucho más pequeños que en la regresión lineal, lo que indica que el modelo está correctamente regularizado. Se aprecia, como se consiguen unas métricas mucho mejores en el conjunto de validación, y por tanto, el modelo regularizado generaliza mejor.\n",
    "\n",
    "Sin embargo, esta ha sido una primera aproximación, vamos a escoger el mejor hiperparámetro alpha para el conjunto de validación, y posteriormente, con el mejor modelo vamos a realizar validación cruzada para obtener una medida robusta del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros a probar\n",
    "hyperparams_dict = {'alpha': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "results, best_hyperparams = hyperparams_evaluation(Ridge, X_train, y_train, X_val, y_val, hyperparams_dict, metric_function=mean_absolute_error, show_iteratioins=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La búsqueda de hiperparámetros ha definido que el mejor valor de alpha es el de 100. Vamos a crear un modelo con dicho valor y vamos a evaluarlo en validación cruazada para obtener una medida robusta de rendimiento de modelo que comparar a posteriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos validación cruzada en el conjunto de entrenamiento más validación\n",
    "ridge = Ridge(alpha=100)\n",
    "mse, mae, r2 = cross_validation(ridge, pd.concat([X_train, X_val]), pd.concat([y_train, y_val]), n_splits=10)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"Validación cruzada:\")\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia vemos cómo funcionaría el modelo con parámetros por defecto con nuestro conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargamos el modelo Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Creamos el modelo, entrenamos con el conjunto de entrenamiento\n",
    "# y validamos con el conjunto de validación\n",
    "rfreg = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos con el conjunto de entrenamiento\n",
    "y_pred = rfreg.predict(X_train)\n",
    "\n",
    "# Calculamos las métricas de rendimiento con el conjunto de entrenamiento\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"--CONJUNTO DE ENTRENAMIENTO--\")\n",
    "print(\"Métricas del modelo por defecto\")\n",
    "print(\" - MAE: \", mae)\n",
    "print(\" - MSE: \", mse)\n",
    "print(\" - RMSE: \", rmse)\n",
    "print(\" - R2: \", r2,\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Predecimos con el conjunto de validación\n",
    "y_pred = rfreg.predict(X_val)\n",
    "\n",
    "# Calculamos las métricas de rendimiento con el conjunto de validación\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"--CONJUNTO DE VALIDACION--\")\n",
    "print(\"Métricas del modelo por defecto\")\n",
    "print(\" - MAE: \", mae)\n",
    "print(\" - MSE: \", mse)\n",
    "print(\" - RMSE: \", rmse)\n",
    "print(\" - R2: \", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras este primer acercamiento a los modelos ensemble, veremos el rendimiento de este modelo aplicando un ajuste de hiperparámetros con el conjunto de validación para poder quedarnos con un mejor modelo. Los hiperparámetros que hemos elegido ajustar son los siguientes:\n",
    "\n",
    "\n",
    "- *n_estimators*: Número de árboles a emplear en el modelo ensemble.\n",
    "\n",
    "- *criterion*: Función que medirá la calidad de la división. Cuyos valores son:\n",
    "    - Error cuadrático \n",
    "    - Error absoluto\n",
    "    - Error cuadrático medio con la mejora de Friedman\n",
    "    - Reducción de Poisson <p>\n",
    "- *max_depth*: Máxima profundida que podrá obtener cada árbol.\n",
    "\n",
    "- *max_leaf_nodes*: Máximo número de Hojas. Su valor *None* se define como número ilimitado de hojas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la función de ajuste de hiperparámetros con los hiperparámetros comentados\n",
    "hyperparams_dict = {'n_estimators': [50,100, 200, 300], 'max_depth': [5, 10, 15, None], 'criterion': ['squared_error','absolute_error','friedman_mse','poisson'], 'max_leaf_nodes': [5, 10, 15, None]}\n",
    "results, best_hyperparams = hyperparams_evaluation(RandomForestRegressor, X_train, y_train, X_val, y_val, hyperparams_dict, metric_function=mean_absolute_error, show_iteratioins=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, aplicaremos validación cruzada a ese último modelo elegido con los mejores hiperparámetros para verificar que estos parámetros realmente son buenos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos validación cruzada en el conjunto de entrenamiento más validación con los hiperparámetros obtenidos\n",
    "rfreg = RandomForestRegressor(n_estimators=100, max_depth=15, criterion='squared_error', max_leaf_nodes=None)\n",
    "mse, mae, r2 = cross_validation(rfreg, pd.concat([X_train, X_val]), pd.concat([y_train, y_val]), n_splits=10)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"Validación cruzada:\")\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos gradient boosting con los datos de entrenamiento\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "gbr = GradientBoostingRegressor(random_state=41)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos los valores de la variable objetivo para los datos de entrenamiento\n",
    "y_train_pred = gbr.predict(X_train)\n",
    "\n",
    "# Predecimos los valores de la variable objetivo para los datos de validación\n",
    "y_val_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluamos el rendimiento del modelo con los datos de entrenamiento\n",
    "print(\"--CONJUNTO DE ENTRENAMIENTO--\")\n",
    "print(\"Rendimiento del modelo por defecto:\")\n",
    "print(\" - MAE: \", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\" - MSE: \", mean_squared_error(y_train, y_train_pred))\n",
    "print(\" - RMSE: \", np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "print(\" - R2: \", r2_score(y_train, y_train_pred), \"\\n\\n\")\n",
    "\n",
    "# Evaluamos el rendimiento del modelo con los datos de validación\n",
    "print(\"--CONJUNTO DE VALIDACION--\")\n",
    "print(\"Rendimiento del modelo por defecto:\")\n",
    "print(\" - MAE: \", mean_absolute_error(y_val, y_val_pred))\n",
    "print(\" - MSE: \", mean_squared_error(y_val, y_val_pred))\n",
    "print(\" - RMSE: \", np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "print(\" - R2: \", r2_score(y_val, y_val_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hiperparámetros que hemos elegido ajustar son los siguientes:\n",
    "\n",
    "- *loss*: Función de pérdida a optimizar. Valores:\n",
    "    - error cuadrático para regresión\n",
    "    - error absoluto\n",
    "    - huber: combinación de las dos anteriores<p>\n",
    "\n",
    "\n",
    "- *learning rate*: factor de aprendizaje que hará que se aprenda más rápido o más lento en nuestro modelo.\n",
    "\n",
    "- *n_estimators*: Número de árboles a emplear en el modelo ensemble.\n",
    "\n",
    "- *criterion*: Función que medirá la calidad de la división. Cuyos valores son:\n",
    "    - Error cuadrático \n",
    "    - Error cuadrático medio con la mejora de friedman<p>\n",
    "\n",
    "- *max_depth*: Máxima profundida que podrá obtener cada árbol.\n",
    "\n",
    "- *max_leaf_nodes*: Máximo número de Hojas. Su valor *None* se define como número ilimitado de hojas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_dict = {'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'], 'learning_rate': [0.01, 0.1, 1], 'n_estimators': [50, 100, 200], 'criterion': ['squared_error', 'friedman_mse'], 'max_depth': [3, None], 'max_leaf_nodes': [5, 10, None]}\n",
    "results, best_hyperparams = hyperparams_evaluation(GradientBoostingRegressor, X_train, y_train, X_val, y_val, hyperparams_dict, metric_function=mean_absolute_error, show_iteratioins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_reg = GradientBoostingRegressor(loss='huber', learning_rate=0.1, n_estimators=200, criterion='squared_error', max_depth=None, max_leaf_nodes=5)\n",
    "mse, mae, r2 = cross_validation(grd_reg, pd.concat([X_train, X_val]), pd.concat([y_train, y_val]), n_splits=10)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"Validación cruzada:\")\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Final\n",
    "\n",
    "Tras realizar un análisis exahustivo de los modelos, hemos llegado a la conclusión de que el modelo que mejor se ajusta a nuestro problema es el **Gradient Boosting**. Este modelo ha obtenido los mejores resultados en las métricas de evaluación en la validación cruzada, por lo que su capacidad de generalización parece ser la mejor comparada con los otros tres modelos analizados.\n",
    "\n",
    "Hemos recogido en una tabla, a modo de resumen, las diferentes métricas obtenidas al usar validación cruazada sobre los modelos ajustados con los mejores hiperparámetros obtenidos en nuestro proceso de búsqueda de hiperparámetros.\n",
    "\n",
    "### Tabla comparativa de resultados\n",
    "|                            | MSE          | RMSE          | MAE           | R2         |\n",
    "|----------------------------|--------------|---------------|---------------|------------|\n",
    "| Regresión lineal           |1.5287+32     | 1.2364+16     | 6.0532e+14    | 0.5164     |\n",
    "| Regresión lineal con Ridge |5.6060e+8     | 23677         | 16341         | 0.8804     |\n",
    "| Random Forest              |6.2548e+8     | 25009         | 16453         | 0.8546     |\n",
    "| **Gradient Boosting**      |**5.0203e+8** | **22406**     | **14313**     | **0.8914** |\n",
    "\n",
    "Se puede ver que el Gradient Boosting es el método que mejores valores tiene para todas las métricas analizadas, por lo que es el modelo que escogemos como final.\n",
    "\n",
    "### Análisis del modelo final\n",
    "Vamos a evaluar este modelo final, para devolver así un resultado realista de su rendimiento sobre nuevas instancias que podrían venir de su implementación en producción. Para ello, vamos a entrenar el modelo con todos los datos de entrenamiento y validación, para obtener toda la información posible de los datos, y vamos a evaluarlo sobre el conjunto de test, un conjunto que hasta ahora no jabíamos usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = GradientBoostingRegressor(loss='huber', learning_rate=0.1, n_estimators=200, criterion='squared_error', max_depth=None, max_leaf_nodes=5)\n",
    "X_train_final = pd.concat([X_train, X_val])\n",
    "y_train_final = pd.concat([y_train, y_val])\n",
    "\n",
    "# Entrenamos el modelo final con los datos de entrenamiento\n",
    "final_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Predecimos los valores de la variable objetivo para los datos de entrenamiento\n",
    "y_train_pred = final_model.predict(X_train_final)\n",
    "\n",
    "# Predecimos los valores de la variable objetivo para los datos de test\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluamos el rendimiento del modelo con los datos de entrenamiento\n",
    "print(\"--CONJUNTO DE ENTRENAMIENTO FINAL (train + validation) --\")\n",
    "print(\"Rendimiento del modelo final:\")\n",
    "print(\" - MAE: \", mean_absolute_error(y_train_final, y_train_pred))\n",
    "print(\" - MSE: \", mean_squared_error(y_train_final, y_train_pred))\n",
    "print(\" - RMSE: \", np.sqrt(mean_squared_error(y_train_final, y_train_pred)))\n",
    "print(\" - R2: \", r2_score(y_train_final, y_train_pred), \"\\n\\n\")\n",
    "\n",
    "# Evaluamos el rendimiento del modelo con los datos de test\n",
    "print(\"--CONJUNTO DE TEST--\")\n",
    "print(\"Rendimiento del modelo final:\")\n",
    "print(\" - MAE: \", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\" - MSE: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\" - RMSE: \", np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "print(\" - R2: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devolvemos estas métricas como rendimiendo fiable del modelo:\n",
    "|                            | MSE          | RMSE          | MAE           | R2         |\n",
    "|----------------------------|--------------|---------------|---------------|------------|\n",
    "| **Gradient Boosting**      |**3.8408e+8** | **19598**     | **13560**     | **0.9334** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos un gráfico de dispersión de los valores reales frente a los predichos\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "plt.xlabel('y real (test)')\n",
    "plt.ylabel('y predicha (test)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"MAE / media de los datos: \", mean_absolute_error(y_test, y_test_pred) / np.mean(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el modelo generaliza de forma aceptable, habiendo obtenido un R2 de 0.9334, lo que indica que el modelo es capaz de explicar el 93.34% de la variabilidad de los datos. Además, el error absoluto medio es de 13560, lo que indica que el modelo se equivoca de media en 13560 dólares al predecir el precio de venta de una vivienda, lo cuál comparado con la magnitud de los precios de venta, es un error aceptable. De hecho, podemos ver, en la última métrica mostrada, que el MAE supone un error de un 7% sobre el precio medio de venta de las viviendas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
