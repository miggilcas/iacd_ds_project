{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - EDA and models comparison\n",
    "\n",
    "*Autores: David Tejero Ruiz & Miguel Gil Castilla*\n",
    "\n",
    "El [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) ha sido obtenido de la página web de Kaggle en el apartado de competiciones. Este ha sido escogido debido a que está recomendado como un dataset de \"juguete\" con el que continuar el aprendizaje en python y ciencia del dato a un nivel más o menos básico. Nuestra idea es realizar un exploratory data análisis aplicando técnicas vistas en clase y ampliando estas con algunas ideas captadas de la red, tras esto trataremos nuestro dataset ya modificado para crear un modelo de regresión con el que predecir la variable respuesta *PriceSale*.\n",
    "\n",
    "Una particularidad de este dataset es su elevado número de atributos (79), lo que lo hace muy adecuado para poner en práctica algunas técnicas vistas de ingeniería de características. Así, podemos extraer información relevante de ellas para realizar una predicción más o menos precisa del precio de venta de las diferentes viviendas.\n",
    "\n",
    "\n",
    "Dado que en el conjunto test.csv no se proporciona el valor de la variable respuesta (era objetivo obtenerlo para la competición de Kaggle al que pertenece), nos hemos centrado en el conjunto *train.csv*, que hemos renombrado al archivo *data.csv* que se encuentra en este directorio, y será el conjunto de datos que analicemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Importamos el conjunto de datos\n",
    "import pandas as pd\n",
    "House_prices = pd.read_csv('data.csv')\n",
    "\n",
    "# Para evitar avisos innecesarios (FutureWarnings):\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que Pandas por defecto, trata los valores NA (Not Available) como NaN (Not a Number). Si se analiza la descripción de los datos, NA es un valor que pueden tomar algunos atributos categóricos (no todos), por lo que no se trata de un valor perdido como tal, sino que  indica que no se puede calcular el valor del atributo, porque la vivienda no dispone de el item que se está evaluando. Por ejemplo, en el caso del atributo de calidad de la piscina, NA indica que la vivienda no dispone de piscina, lo cuál es información relevante, y no un valor perdido que tendríamos que imputar.\n",
    "\n",
    "Hemos analizado la descripción de los diferentes atributos categóricos y aquellos en los que un NA podría aportar información relevante, son:\n",
    "\n",
    "- Alley\n",
    "\n",
    "- BsmtQual\n",
    "\n",
    "- BsmtCond\n",
    "\n",
    "- BsmtExposure\n",
    "\n",
    "- BsmtFinType1\n",
    "\n",
    "- BsmtFinType2\n",
    "\n",
    "- FireplaceQu\n",
    "\n",
    "- GarageType\n",
    "\n",
    "- GarageYrBlt\n",
    "\n",
    "- GarageFinish\n",
    "\n",
    "- GarageQual\n",
    "\n",
    "- GarageCond\n",
    "\n",
    "- PoolQC\n",
    "\n",
    "- Fence\n",
    "\n",
    "- MiscFeature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_attributes = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                 'PoolQC', 'Fence', 'MiscFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la cabecera de los datos\n",
    "House_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el dataset está compuesto por 81 columnas, de las cuales 79 serán las características que usaremos en la predicción, y las dos restantes sol el id (que no aporta información), y la variable que queremos predecir (precio de venta de la casa).\n",
    "\n",
    "Además notamos que existe una riqueza de características alta, encontrando tanto valores numéricos como categóricos, cosa que analizaremos a continuación en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a definir un problema de regresión sobre el precio de las casas, por lo que la variable \n",
    "# objetivo del dataset será 'SalePrice' (última columna de nuestro dataset como es habitual)\n",
    "y_data = House_prices['SalePrice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de missing values y non-available values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentamos al inicio, el dataset contiene valores NA, y en algunos casos realmente podrían aportar información útil al decir que no se tiene el objeto que tratamos de evaluar en el atributo. Vamos a ver qué atributos contienen valores NA, y cuántos de estos valores NA contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a contar las apariciones de NaNs en nuestro dataset al completo\n",
    "print(\"Hay \", House_prices.isnull().sum().sum(), \" valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el conjunto de datos en atributos con posibles valores NA y \n",
    "# atributos con posibles missing values\n",
    "House_prices_na = House_prices[na_attributes]\n",
    "House_prices_mv = House_prices.drop(na_attributes, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuales de los atributos en los que un NaN significa Missing Value \n",
    "# tienen realmente valores NaN\n",
    "mv_counts = House_prices_mv.isnull().sum()\n",
    "mv_counts = mv_counts[mv_counts > 0]\n",
    "print(\"Atributos que contienen missing values: \\n{}\".format(mv_counts))\n",
    "print(\"Hay \", len(mv_counts), \" atributos que contienen missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De entre aquellos en los que NA no es un valor posible del atributo,\n",
    "# y las apariciones de NaN son datos pérdidos, vamos a ver cuáles son \n",
    "# numéricos y cuáles no\n",
    "House_prices_mv_numerics = House_prices_mv[mv_counts.keys()].select_dtypes(include=[np.number])\n",
    "House_prices_mv_categorics = House_prices_mv[mv_counts.keys()].select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen missing values: \\n{}\".format(House_prices_mv_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen missing values: \\n{}\".format(House_prices_mv_categorics.keys().to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos numéricos que contienen missing values los vamos a rellenar con la media\n",
    "# de los valores de la columna\n",
    "House_prices_mv_numerics = House_prices_mv_numerics.fillna(House_prices_mv_numerics.mean())\n",
    "\n",
    "# Los atributos categóricos que contienen missing values los vamos a rellenar con el valor\n",
    "# más frecuente de la columna\n",
    "House_prices_mv_categorics = House_prices_mv_categorics.fillna(House_prices_mv_categorics.mode().iloc[0])\n",
    "\n",
    "# Unimos los dos conjuntos de datos\n",
    "House_prices_mv_fixed = pd.concat([House_prices_mv_numerics, House_prices_mv_categorics], axis=1)\n",
    "\n",
    "# Guardamos los datos tratados\n",
    "for key in mv_counts.keys():\n",
    "    House_prices[key] = House_prices_mv_fixed[key]\n",
    "\n",
    "# Vamos a comprobar que ya no hay valores nulos en los atributos que hemos tratado\n",
    "if (House_prices[mv_counts.keys()].isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Available values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que atributos contienen valores NA entre los que NA aporta información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuales de los atributos en los que un NaN significa Missing Value \n",
    "# tienen realmente valoress NaN\n",
    "na_counts = House_prices_na.isnull().sum()\n",
    "na_counts = na_counts[na_counts > 0]\n",
    "print(\"Atributos que contienen Non Available values: \\n{}\".format(na_counts))\n",
    "print(\"Hay \", len(na_counts), \" atributos que contienen Non Available values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na.select_dtypes(include=[np.number])\n",
    "House_prices_na_categorics = House_prices_na.select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen Non Available values: \\n{}\".format(House_prices_na_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen Non Available values: \\n{}\".format(House_prices_na_categorics.keys().to_list()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar tratando el caso especial del único atributo numérico que tiene un NA como valor informativo, hablamos del **GarageYrBlt**. Este atributo codifica el año en el que se construyó el garaje de la casa, y un valor NA indica que la casa no tiene garaje. La información de \"no tenemos garaje\" puede ser relevante, por lo que tenemos que tratar este caso cuidosamente.\n",
    "\n",
    "Sin embargo, parte de los atributos que analizaremos un poco más adelante: GarageType, GarageFinish, GarageCond y Garageal, son atributos categóricos cuyo valor NA indica igualmente \"no tenemos garaje\", por lo que esa información en este atributo en particular es redundante.\n",
    "\n",
    "Por tanto, hemos llegado a la decisión de sustituir los valores NA de GarageYrBlt por la media de los valores no NA de este atributo. De esta forma, cuando posteriormente le apliquemos una normalización estándar al atributo y le restemos la media, obtendremos un valor nulo, y por tanto no afectará a la predicción dado a que así indicaremos que no es relevante ese valor en este atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na_numerics.fillna(House_prices_na_numerics.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el resto de valores NA, los NaNs indican un posible valor categorico, por lo que vamos a sustituirlos por la cadena \"None\", para que se traten como un posible valor categórico más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos los NaNs con la cadena 'None\n",
    "House_prices_na_categorics = House_prices_na_categorics.fillna('None')\n",
    "\n",
    "# Unimos los dos conjuntos de datos tratados\n",
    "House_prices_na_fixed = pd.concat([House_prices_na_numerics, House_prices_na_categorics], axis=1)\n",
    "\n",
    "# Guardamos los datos tratados\n",
    "for key in na_counts.keys():\n",
    "    House_prices[key] = House_prices_na_fixed[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a comprobar que ya no hay valores nulos en nuestro conjunto de datos\n",
    "if (House_prices.isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase inicial de ingeniería de características\n",
    "Vamos a analizar estas sustituticiones con los valores 'None' que repercusión tiene sobre el conjunto de datos. Se mostrará una gráfica con la cantidad de estos valores para valorar si es necesario seguir con la exploración o simplemente eliminar las estancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a graficar los valores NA que susituimos por 'None'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ordenados_na = na_counts.sort_values(ascending=False)\n",
    "plt.bar(ordenados_na.index, ordenados_na)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Número de valores NA')\n",
    "plt.xlabel('Atributos')\n",
    "plt.title('Valores NA sustituidos por None')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print de los 4 mayores:\n",
    "print(\"Los 4 mayores valores NA sustituidos por None son:\")\n",
    "print(ordenados_na.head(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que existe una cantidad exuberante de datos marcados con None, superando el 50% de las instancias, para 4 de los atributos, concretamente:\n",
    " - PoolQC: 1453\n",
    " - MiscFeature: 1406 \n",
    " - Alley: 1369\n",
    " - Fence: 1179\n",
    " \n",
    "Debemos de considerar que el número total de instancias en el dataset es de **1460**.\n",
    "\n",
    "Para ello dibujaremos el par atributo-variable respuesta para ver la relación entre estos dos, concretamente en los valores que no son None. Con esta comparación veremos si es relevante la información que nos aporta o no el atributo, decidiendo si eliminarlo o no.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos histograma de la variable objetivo, para ver su distribución\n",
    "plt.hist(y_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da la intuición de dónde se sitúa la media de la variable respuesta y somo es su distribución, con ella podremos decidir si eliminar o no algunso atributos. Nuestra idea es que si los valores que no son 'None' son pocos y se sitúan cercanos a la donde de encuentran la mayoría de instancias (cercanos a la media de la variable respuesta), probablemente estos atributos no aportarán información relevante para la regresión, comparado con la información de otros atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los valores  de las 4 variables con mayor número de valores NA\n",
    "# en aquellos que son diferentes de None.\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "ax[0,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[0]])\n",
    "ax[0,0].set_xlabel('SalePrice')\n",
    "ax[0,0].set_ylabel(ordenados_na.index[0])\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[1]])\n",
    "ax[0,1].set_xlabel('SalePrice')\n",
    "ax[0,1].set_ylabel(ordenados_na.index[1])\n",
    "ax[0,1].grid()\n",
    "\n",
    "ax[1,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[2]])\n",
    "ax[1,0].set_xlabel('SalePrice')\n",
    "ax[1,0].set_ylabel(ordenados_na.index[2])\n",
    "ax[1,0].grid()\n",
    "\n",
    "ax[1,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[3]])\n",
    "ax[1,1].set_xlabel('SalePrice')\n",
    "ax[1,1].set_ylabel(ordenados_na.index[3])\n",
    "ax[1,1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en las gráficas comparativas, se observa que los atributos toman 'None' a lo largo de todo el rango de valores de SalePrice. Vemos que los valores que no son None de los atributos categóricos son pocos y se encuentran además en la zona donde tenemos un alto número de instancias, con lo cual, no aportan información relevante en este problema de regresión. Es por esto por lo que decidimos directamente eliminar estos atributos categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los 4 atributos que más NA contienen \n",
    "House_prices = House_prices.drop(ordenados_na.head(4).index, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de nuestro dataset\n",
    "Tras haber realizado un primer análisis inicial, vamos a construir el dataset de partida, que será el que usaremos para realizar el análisis exploratorio de datos y la construcción de los modelos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el dataset de datos de entrada (X_data) como el dataset sin la variable objetivo\n",
    "# y sin la variable 'Id' (que no aporta información)\n",
    "X_data = House_prices.drop(['SalePrice', 'Id'], axis=1)\n",
    "\n",
    "# Para tratar los diferentes atributos lo primero es distinguir entre las variables cuantitativas \n",
    "# (numéricas) y cualitativas (categóricas)\n",
    "# Ya que el tipo de dato 'object' engloba a las variables categóricas y a las variables de tipo\n",
    "# # string, se puede usar dicho tipo de dato para distinguir las variables en numéricas y categóricas\n",
    "numerical_atr = [col for col in X_data.columns if X_data.dtypes[col] != 'object']\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "\n",
    "print(\"Hay \", len(numerical_atr), \" datos numéricos: \",numerical_atr)\n",
    "print(\"Hay \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las variables categoricas\n",
    "X_data[categorical_atr].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como bien sabemos, estos atributos categóricos debemos transformarlos a valores numéricos para poder trabajar con ellos. Existen diferentes técnicas para realizar esto, y para hacerlo de forma consistente, debemos analizar la naturaleza de los diferentes atributos categóricos.\n",
    "\n",
    "Notamos que existen una serie de variables categóricas a cuyos atributos podemos asignarles un valor numérico según un orden. Estos casos son cuando hablamos de calidades (a mayor calidad vamos a asignar un valor superior) o cuando cuantificamos con palabras una cantidad (por ejemplo, el LandSlope caracteriza la pendiente del terreno, y podemos asignarle un valor numérico mayor a una mayor pendiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basándonos en el archivo data_description.txt codificamos las variables categóricas con información cuantitativa\n",
    "X_data['LotShape'] = X_data['LotShape'].map({'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0})\n",
    "X_data['LandSlope'] = X_data['LandSlope'].map({'Gtl': 0, 'Mod': 1, 'Sev': 2})\n",
    "X_data['ExterQual'] = X_data['ExterQual'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['ExterCond'] = X_data['ExterCond'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['BsmtQual'] = X_data['BsmtQual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['BsmtCond'] = X_data['BsmtCond'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['BsmtExposure'] = X_data['BsmtExposure'].map({'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0})\n",
    "X_data['BsmtFinType1'] = X_data['BsmtFinType1'].map({'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['BsmtFinType2'] = X_data['BsmtFinType2'].map({'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['HeatingQC'] = X_data['HeatingQC'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['KitchenQual'] = X_data['KitchenQual'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['Functional'] = X_data['Functional'].map({'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0})\n",
    "X_data['FireplaceQu'] = X_data['FireplaceQu'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['GarageFinish'] = X_data['GarageFinish'].map({'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['GarageQual'] = X_data['GarageQual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['GarageCond'] = X_data['GarageCond'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['PavedDrive'] = X_data['PavedDrive'].map({'Y': 2, 'P': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas dos características podemos tratarlas como binarias al tener solo dos posibles valores\n",
    "X_data['CentralAir'] = X_data['CentralAir'].map({'Y': 1, 'N': 0})\n",
    "X_data['Street'] = X_data['Street'].map({'Pave': 1, 'Grvl': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las que nos quedan, son variables categóricas que no podemos ordenar, y por tanto, debemos aplicar una codificación one-hot. Para ello, vamos a usar la función OneHotEncoder de sklearn, que nos permite realizar esta codificación de forma sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculamos los valores que siguen siendo categóricos\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "print(\"Quedan \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Usamos OneHotEncoder para codificar las variables categóricas\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_data_encoded = pd.DataFrame(encoder.fit_transform(X_data[categorical_atr]))\n",
    "X_data_encoded.columns = encoder.get_feature_names(categorical_atr)\n",
    "\n",
    "# Borramos las variables categóricas originales y añadimos las nuevas codificadas\n",
    "X_data = X_data.drop(categorical_atr ,axis=1)\n",
    "X_data = pd.concat([X_data, X_data_encoded], axis=1)\n",
    "\n",
    "# Mostramos los datos de entrada\n",
    "X_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencionar aunque 214 parezcan muchos atributos, gracias al label encoding manual que hemos realizado arriba usando información de la descripción de los datos, nos hemos ahorrado hacer one-hot-encoding de todos los atributos categóricos, lo cual supondría obtener bastantes más atributos, aumentando consideralmente la complejidad del problema.\n",
    "\n",
    "Notar que hemos optado por usar one-hot-encoding para las variables en las que no podemos definir un orden, y no otro tipo de codificaciones, para no introducir un orden artificial que no existiría en la realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos que no hay valores nulos en el conjunto final de datos\n",
    "if (X_data.isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")\n",
    "\n",
    "# Probamos que no quedan variables categóricas\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "print(\"Quedan \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingeniería de características avanzada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenermos la matriz de correlación de los datos de entrada\n",
    "correlation_matrix = X_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de correlación de Pearson entre los atributos\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "sns.heatmap(correlation_matrix, cmap='viridis',vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a identificar pares de atributos con correlación mayor a 0.7\n",
    "# y vamos a eliminar uno de los dos atributos de cada par\n",
    "# para evitar la multicolinealidad\n",
    "\n",
    "# Mantenemos una lista de atributos eliminados para no eliminarlos dos veces, y simplifcamos\n",
    "\n",
    "# La matriz es simétrica, por lo que definimos los bucles para recorrer solo la mitad superior\n",
    "# de la matriz\n",
    "\n",
    "# Tenemos en cuenta tanto correlación positiva como negativa, por lo que usamos el valor absoluto\n",
    "\n",
    "umbral = 0.7\n",
    "atributos_eliminados = []\n",
    "for i in range(len(correlation_matrix.columns)): \n",
    "    for j in range(i+1, len(correlation_matrix.columns)): \n",
    "        if (np.abs(correlation_matrix.iloc[i, j]) >= umbral):\n",
    "            if (correlation_matrix.columns[j] not in atributos_eliminados):\n",
    "                atributo = correlation_matrix.columns[j]\n",
    "                atributos_eliminados.append(atributo)\n",
    "                print(\"Eliminamos el atributo \", atributo, \" por tener una correlación mayor a \", umbral, \" con el atributo \", correlation_matrix.columns[i])\n",
    "\n",
    "print(\"Se van a eliminar \", len(atributos_eliminados), \" atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los atributos de la lista de atributos a eliminar\n",
    "X_data = X_data.drop(atributos_eliminados, axis=1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metodos de filtrado para selección de características\n",
    "A continuación, vamos a estudiar la correlación de los atributos con la variable respuesta, para ver si podemos eliminar algunos atributos que no aporten información relevante para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.DataFrame(pd.concat([X_data, y_data], axis=1)).corr()\n",
    "correlation_matrix = correlation_matrix['SalePrice']\n",
    "\n",
    "# Ploteamos la matriz de correlación de los datos de entrada\n",
    "plt.figure(figsize=(15, 0.5))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "sns.heatmap(correlation_matrix.to_frame().T, cmap='viridis', vmax=1, vmin=-1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a eliminar los atributos que tengan una correlación pequeña con la variable objetivo\n",
    "umbral = 0.025\n",
    "atributos_a_eliminar = []\n",
    "for i in range(len(correlation_matrix)):\n",
    "    if (np.abs(correlation_matrix[i]) < umbral):\n",
    "        atributo = correlation_matrix.index[i]\n",
    "        atributos_a_eliminar.append(atributo)\n",
    "        print(\"Eliminamos el atributo \", atributo, \" por tener una correlación menor a \", umbral, \" con la variable objetivo\")\n",
    "\n",
    "print(\"Se van a eliminar \", len(atributos_a_eliminar), \" atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los atributos de la lista de atributos a eliminar\n",
    "X_data = X_data.drop(atributos_a_eliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Outliers\n",
    "Vamos a añadir un análisis de outliers, para ver si podemos eliminar algunas intancias que no aporten información relevante para la predicción.\n",
    "\n",
    "\\TODO explicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = {}\n",
    "for key in X_data.keys():\n",
    "    q1 = X_data[key].quantile(0.25)\n",
    "    q3 = X_data[key].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    umbral_inferior = q1 - 1.5 * iqr\n",
    "    umbral_superior = q3 + 1.5 * iqr\n",
    "    outliers_encontrados = X_data[(X_data[key] < umbral_inferior) | (X_data[key] > umbral_superior)].index\n",
    "    outliers[key] = outliers_encontrados\n",
    "\n",
    "# Vamos a identificar los outliers que aparezcan en un minimo de atributos\n",
    "# y vamos a eliminarlos\n",
    "minimo_atributos_para_eliminar = 5\n",
    "indices_outliers = []\n",
    "contador_instancias = {}\n",
    "\n",
    "# Contar el número de ocurrencias de cada instancia\n",
    "for atributo, indices in outliers.items():\n",
    "    for indice in indices:\n",
    "        if indice not in contador_instancias:\n",
    "            contador_instancias[indice] = 1\n",
    "        else:\n",
    "            contador_instancias[indice] += 1\n",
    "\n",
    "# Identificar los índices de las instancias que son outliers en un número mínimo de atributos\n",
    "for instancia, contador in contador_instancias.items():\n",
    "    if contador >= minimo_atributos_para_eliminar:\n",
    "        indices_outliers.append(instancia)\n",
    "\n",
    "# Eliminamos los outlier\n",
    "X_data = X_data.drop(indices_outliers, axis=0)\n",
    "y_data = y_data.drop(indices_outliers, axis=0)\n",
    "\n",
    "print(\"Se han eliminado \", len(indices_outliers), \" instancias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "# z_scores = np.abs(stats.zscore(X_data))\n",
    "# threshold = 3\n",
    "# X_data = X_data[(z_scores < threshold).all(axis=1)]\n",
    "# y_data = y_data[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "# print(\"Se han eliminado \", len(z_scores) - len(X_data), \" instancias\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el conjunto de datos en tres subconjuntos: entrenamiento, validación y test\n",
    "# En una proprorción 50% - 20% - 30% respectivamente\n",
    "\n",
    "train_size = 0.7\n",
    "val_size = 0.2\n",
    "test_size = 0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Realizamos esta división en train y test para hacer validación cruzada con un conjunto de entrenamiento que será dividido en folds y un conjunto de test \n",
    "# que será usado para evaluar el modelo final como datos nunca vistos\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(X_data, y_data, test_size=(1-train_size), random_state=41)\n",
    "# Dividimos el conjunto auxiliar en validación y test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=test_size/(test_size + val_size), random_state=41)\n",
    "\n",
    "# Mostramos los tamaños de los conjuntos de datos\n",
    "print(\"Tamaño del conjunto de entrenamiento: \", len(X_train))\n",
    "print(\"Tamaño del conjunto de test: \", len(X_test))\n",
    "print(\"Tamaño del conjunto de validación: \", len(X_val))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las variables numéricas para que todas tengan media 0 y desviación típica 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_data.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_data.columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_data.columns)\n",
    "\n",
    "# Mostramos los datos de entrada\n",
    "X_train.head()\n",
    "\n",
    "X_data = pd.concat([X_train, X_val, X_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos\n",
    "Los modelos escogidos para realizar el entrenamiento y que más tarde pondremos a prueba serán:\n",
    "- Regresión Lineal\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "\n",
    "## Proceso de elección del modelo de regresión\n",
    "Para escoger el modelo de regresión, vamos a realizar los siguientes pasos:\n",
    "1. Dividimos los datos en **tres** conjuntos: entrenamiento, validación y test.\n",
    "2. Escogemos un modelo de regresión entre los que hemos mencionado anteriormente.\n",
    "3. Entrenamos dicho modelo con los datos de entrenamiento y realizamos un **ajuste de sus hiperparámetros** con los datos de validación.\n",
    "4. Finalmente, para el conjunto con \"mejores\" hiperparámetros obtenidos, realizaremos **validación cruzada** uniendo los conjuntos de entrenamiento y validación, obteniendo una medida robusta de las métricas de rendimiento del modelo.\n",
    "5. Repetimos estos pasos anteriores para cada uno de los modelos de regresión que queremos comparar.\n",
    "6. Escogemos el modelo que mejores métricas tenga en la validación cruzada, lo entrenamos con todo el conjunto de datos de entrenamiento y validación, y lo **evaluamos en el conjunto de test**, que no hemos usado hasta ahora, para dar así una medida de rendimiento realista del modelo final.\n",
    "\n",
    "\n",
    "## Métricas\n",
    "Las métricas de evaluación del rendimiento del modelo que elegimos son las siguientes:\n",
    "\n",
    "- Error absoluto medio (MAE): Esta métrica se define como el error medio, es decir, variable respuesta - valor predicho : $MAE = \\frac{\\sum|y-\\hat y|}{N}$ (donde N es el número de instancias). Este nos da el error en la misma unidad que la variable respuesta, con lo cual es bastante intuitivo. En cambio, no es derivable con lo que habría que usar métodos como el descenso por el gradiente como optimizador para el entrenamiento del modelo. \n",
    "\n",
    "- Error cuadrático medio (MSE): Esta métrica se define como: $MSE = \\frac{\\sum(y-\\hat y)²}{N}$. Este fue el que utilizamos en una primera instancia y confundió nuestro entendimiento de los resultados al tener valores tan elevados. Esto es debido a que evidentemente es por su valor cuadrático, aún así es interesante porque puede usarse como función de pérdida.\n",
    "\n",
    "- La métrica R2: también llamada coeficiente de determinación, es una métrica que indica qué tan bien se ajusta un modelo de regresión a los datos observados, variando entre 0 y 1. Un valor cercano a 1 indica un buen ajuste, mientras que un valor cercano a 0 significa que el modelo no puede explicar la variabilidad de los datos. De hecho, un valor de cero, sería lo que obtendría un modelo que devuelva siempre el valor medio de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las métricas que emplearemos para evaluar el rendimiento de los modelos\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que nuestra cantidad de datos no es muy grande, vamos a usar validación cruzada en lugar del método holdout:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividimos el conjunto de entrenamiento en 5 subconjuntos\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Creamos una función para hacer la validación cruzada\n",
    "def cross_validation(Model, X, y, n_splits=10):\n",
    "\n",
    "    # Aplicamos  shuffle para que los datos se mezclen antes de dividirlos\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    splits = kf.split(X)\n",
    "    \n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    scores_r2 = []\n",
    "\n",
    "\n",
    "    for train_idx, val_idx in splits:\n",
    "        model = Model()\n",
    "        # Extraemos el conjunto de entrenamiento y el conjunto de validación indexando\n",
    "        # con los índices de cada conjunto aportados por la función .split()\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Entrenamos el modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluamos el modelo\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        scores_mae.append(mean_absolute_error(y_pred, y_val))\n",
    "        scores_mse.append(mean_squared_error(y_pred, y_val))\n",
    "        scores_r2.append(r2_score(y_pred, y_val))\n",
    "        #print(\"R2: \", r2_score(y_pred, y_val))\n",
    "        #print(\"MAE: \", mean_absolute_error(y_pred, y_val))\n",
    "        #print(\"MSE: \", mean_squared_error(y_pred, y_val))\n",
    "        # See sales\n",
    "        plt.scatter(y_val, y_pred)\n",
    "        plt.pause(0.05)\n",
    "   \n",
    "    return np.mean(scores_mse), np.mean(scores_mae), np.mean(scores_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "En primer lugar comenzaremos con un modelo lo más simple posible para casos de regresión, el modelo visto en clase *Regresión Lineal*, así tendremos una base con la que comparar modelos más complejos a posteriori. Con esto conseguiremos algo así como un modelo *\"Dummy\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargamos el modelo de regresión lineal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creamos el modelo, entrenamos con el conjunto de entrenamiento \n",
    "# y validamos con el conjunto de validación\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_val)\n",
    "\n",
    "# Calculamos las métricas de rendimiento\n",
    "mae = mean_absolute_error(y_val.to_numpy(), y_pred)\n",
    "mse = mean_squared_error(y_val.to_numpy(), y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val.to_numpy(), y_pred)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)\n",
    "\n",
    "plt.scatter(y_val.to_numpy(), y_pred)\n",
    "plt.xlabel('y_val')\n",
    "plt.ylabel('y_pred')\n",
    "plt.show()\n",
    "\n",
    "# Idetntificamos instancia con mayor error\n",
    "error = np.abs(y_val.to_numpy() - y_pred)\n",
    "max_error = np.max(error)\n",
    "max_error_idx = np.argmax(error)\n",
    "print(\"El mayor error es: \", max_error)\n",
    "print(\"La instancia con mayor error es: \", max_error_idx)\n",
    "print(\"El valor real de la instancia con mayor error es: \", y_val.to_numpy()[max_error_idx])\n",
    "print(\"El valor predicho de la instancia con mayor error es: \", y_pred[max_error_idx])\n",
    "print(\"La instancia con mayor error es: \", X_val.iloc[max_error_idx][:])\n",
    "\n",
    "# Eliminamos la instancia con mayor error del conjunto de validación\n",
    "X_val_aux = X_val.drop(max_error_idx, axis=0)\n",
    "y_val_aux = y_val.drop(max_error_idx, axis=0)\n",
    "\n",
    "y_pred = reg.predict(X_val_aux)\n",
    "\n",
    "# Calculamos las métricas de rendimiento\n",
    "mae = mean_absolute_error(y_val_aux.to_numpy(), y_pred)\n",
    "mse = mean_squared_error(y_val_aux.to_numpy(), y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val_aux.to_numpy(), y_pred)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de regresión lineal, no tiene hiperparámetros relevantes que ajustar, por lo que es no es necesario realizar una búsqueda de hiperparámetros, ya que al fin y al cabo, encontrará el hiperplano que reduzca el error cuadrático medio con los datos de entrenamiento, y el resultado final será el mismo para los mismos datos.\n",
    "\n",
    "Por lo tanto, vamos a obtener una medida de rendimiento de este modelo usando validación cruzada, para poder generar las métricas a comparar con los otros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos validación cruzada en el conjunto de entrenamiento más validación\n",
    "mse, mae, r2 = cross_validation(LinearRegression, pd.concat([X_train, X_val]), pd.concat([y_train, y_val]), n_splits=3)\n",
    "\n",
    "# Mostramos las métricas de rendimiento\n",
    "print(\"Validación cruzada:\")\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print(\"R2: \", r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las métricas de rendimiento\n",
    "print(\"Validación cruzada:\")\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a realizar el proceso mencionado de búsqueda de hiperparámetros que mejor se ajusten a nuestro datos, para ello, vamos a tratar de encontrar los parámetros del modelo que minimizan el error cuadrático medio (MSE) en el conjunto de validación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenamos gradient boosting con los datos de entrenamiento\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "gbr = GradientBoostingRegressor(random_state=41)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predecimos los valores de la variable objetivo para los datos de entrenamiento\n",
    "y_train_pred = gbr.predict(X_train)\n",
    "\n",
    "# Predecimos los valores de la variable objetivo para los datos de validación\n",
    "y_val_pred = gbr.predict(X_val)\n",
    "\n",
    "# Evaluamos el rendimiento del modelo con los datos de entrenamiento\n",
    "print(\"Rendimiento del modelo con los datos de entrenamiento:\")\n",
    "print(\"MAE: \", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "print(\"R2: \", r2_score(y_train, y_train_pred))\n",
    "\n",
    "# Evaluamos el rendimiento del modelo con los datos de validación\n",
    "print(\"Rendimiento del modelo con los datos de validación:\")\n",
    "print(\"MAE: \", mean_absolute_error(y_val, y_val_pred))\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "print(\"R2: \", r2_score(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See sales\n",
    "plt.scatter(y_train, y_pred)\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSAS QUE HAY QUE HACER:\n",
    "\n",
    "**Preprocesado de datos** DONE\n",
    "- Eliminar columnas con muchos valores perdidos\n",
    "- One-hot encoding / Label encoding (categorical variables)\n",
    "- Normalizar datos: StandardScaler, MinMaxScaler, RobustScaler \n",
    "- Eliminar outliers: IQR, Z-score, etc.  (?)\n",
    "- Discretizado de variables continuas (?): Binning, etc.\n",
    "\n",
    "\n",
    "**Ingeneering features** DONE\n",
    "- Reducción de dimensionalidad: *PCA*, LDA, etc.\n",
    "- Selección de variables - Filter Methods: Correlation, Chi2, ANOVA, etc.\n",
    "- Multicolinealidad: VIF, etc. \n",
    "- Crear nuevas variables.\n",
    "\n",
    "**Posibles Modelos**\n",
    "- Regresión lineal\n",
    "- Regresión polinómica\n",
    "- Regresión Ridge\n",
    "- Regresión Lasso\n",
    "- Regresión ElasticNet\n",
    "- SVR\n",
    "- Random Forest\n",
    "- **Gradient Boosting**\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "**Evaluación de modelos**\n",
    "- MSE\n",
    "- k-fold cross validation\n",
    "- Recall, Precision, F1-score, etc. (?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
