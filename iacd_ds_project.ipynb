{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - EDA and models comparison\n",
    "\n",
    "*Autores: David Tejero Ruiz & Miguel Gil Castilla*\n",
    "\n",
    "El [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) ha sido obtenido de la página web de Kaggle en el apartado de competiciones. Este ha sido escogido debido a que está recomendado como un dataset de \"juguete\" con el que continuar el aprendizaje en python y ciencia del dato a un nivel más o menos básico. Nuestra idea es realizar un exploratory data análisis aplicando técnicas vistas en clase y ampliando estas con algunas ideas captadas de la red, tras esto trataremos nuestro dataset ya modificado para crear un modelo de regresión con el que predecir la variable respuesta *PriceSale*.\n",
    "\n",
    "Una particularidad de este dataset es su elevado número de atributos (79), lo que lo hace muy adecuado para poner en práctica algunas técnicas vistas de ingeniería de características. Así, podemos extraer información relevante de ellas para realizar una predicción más o menos precisa del precio de venta de las diferentes viviendas.\n",
    "\n",
    "\n",
    "Dado que en el conjunto test.csv no se proporciona el valor de la variable respuesta (era objetivo obtenerlo para la competición de Kaggle al que pertenece), nos hemos centrado en el conjunto *train.csv*, que hemos renombrado al archivo *data.csv* que se encuentra en este directorio, y será el conjunto de datos que analicemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Importamos el conjunto de datos\n",
    "import pandas as pd\n",
    "House_prices = pd.read_csv('data.csv')\n",
    "\n",
    "# Para evitar avisos innecesarios (FutureWarnings):\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que Pandas por defecto, trata los valores NA (Not Available) como NaN (Not a Number). Si se analiza la descripción de los datos, NA es un valor que pueden tomar algunos atributos categóricos (no todos), por lo que no se trata de un valor perdido como tal, sino que  indica que no se puede calcular el valor del atributo, porque la vivienda no dispone de el item que se está evaluando. Por ejemplo, en el caso del atributo de calidad de la piscina, NA indica que la vivienda no dispone de piscina, lo cuál es información relevante, y no un valor perdido que tendríamos que imputar.\n",
    "\n",
    "Hemos analizado la descripción de los diferentes atributos categóricos y aquellos en los que un NA podría aportar información relevante, son:\n",
    "\n",
    "- Alley\n",
    "\n",
    "- BsmtQual\n",
    "\n",
    "- BsmtCond\n",
    "\n",
    "- BsmtExposure\n",
    "\n",
    "- BsmtFinType1\n",
    "\n",
    "- BsmtFinType2\n",
    "\n",
    "- FireplaceQu\n",
    "\n",
    "- GarageType\n",
    "\n",
    "- GarageYrBlt\n",
    "\n",
    "- GarageFinish\n",
    "\n",
    "- GarageQual\n",
    "\n",
    "- GarageCond\n",
    "\n",
    "- PoolQC\n",
    "\n",
    "- Fence\n",
    "\n",
    "- MiscFeature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_attributes = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                 'PoolQC', 'Fence', 'MiscFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la cabecera de los datos\n",
    "House_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el dataset está compuesto por 81 columnas, de las cuales 79 serán las características que usaremos en la predicción, y las dos restantes sol el id (que no aporta información), y la variable que queremos predecir (precio de venta de la casa).\n",
    "\n",
    "Además notamos que existe una riqueza de características alta, encontrando tanto valores numéricos como categóricos, cosa que analizaremos a continuación en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a definir un problema de regresión sobre el precio de las casas, por lo que la variable \n",
    "# objetivo del dataset será 'SalePrice' (última columna de nuestro dataset como es habitual)\n",
    "y_data = House_prices['SalePrice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentamos al inicio, el dataset contiene valores NA, y en algunos casos realmente podrían aportar información útil al decir que no se tiene el objeto que tratamos de evaluar en el atributo. Vamos a ver qué atributos contienen valores NA, y cuántos de estos valores NA contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a contar las apariciones de NaNs en nuestro dataset al completo\n",
    "print(\"Hay \", House_prices.isnull().sum().sum(), \" valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el conjunto de datos en atributos con posibles valores NA y \n",
    "# atributos con posibles missing values\n",
    "House_prices_na = House_prices[na_attributes]\n",
    "House_prices_mv = House_prices.drop(na_attributes, axis=1)\n",
    "\n",
    "na_counts = House_prices_mv.isnull().sum()\n",
    "na_counts = na_counts[na_counts > 0]\n",
    "print(\"Atributos que contienen missing values: \\n{}\".format(na_counts))\n",
    "print(\"Hay \", len(na_counts), \" atributos que contienen missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De entre aquellos en los que NA no es un valor posible del atributo,\n",
    "# y las apariciones de NaN son datos pérdidos, vamos a ver cuáles son \n",
    "# numéricos y cuáles no\n",
    "House_prices_mv_numerics = House_prices_mv[na_counts.keys()].select_dtypes(include=[np.number])\n",
    "House_prices_mv_categorics = House_prices_mv[na_counts.keys()].select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen missing values: \\n{}\".format(House_prices_mv_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen missing values: \\n{}\".format(House_prices_mv_categorics.keys().to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos numéricos que contienen missing values los vamos a rellenar con la media\n",
    "# de los valores de la columna\n",
    "House_prices_mv_numerics = House_prices_mv_numerics.fillna(House_prices_mv_numerics.mean())\n",
    "\n",
    "# Los atributos categóricos que contienen missing values los vamos a rellenar con el valor\n",
    "# más frecuente de la columna\n",
    "House_prices_mv_categorics = House_prices_mv_categorics.fillna(House_prices_mv_categorics.mode().iloc[0])\n",
    "\n",
    "# Vamos a comprobar que ya no hay valores nulos en nuestro dataset\n",
    "if House_prices_mv_numerics.isnull().sum().sum() == 0 and House_prices_mv_categorics.isnull().sum().sum() == 0:\n",
    "    print(\"No hay valores nulos en estos conjuntos\")\n",
    "\n",
    "# Unimos los dos conjuntos de datos como salida de la imputación de valores perdidos\n",
    "House_prices_mv = pd.concat([House_prices_mv_numerics, House_prices_mv_categorics], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los valores cuyos NaNs indican un posible valor categorico, vamos a sustituirlos por la cadena \"None\", para que no se traten como valores perdidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na.select_dtypes(include=[np.number])\n",
    "House_prices_na_categorics = House_prices_na.select_dtypes(exclude=[np.number])\n",
    "\n",
    "print(\"Atributos numericos que contienen NA: \\n{}\".format(House_prices_na_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen NA: \\n{}\".format(House_prices_na_categorics.keys().to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos a continuación una gráfica la información anterior\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ordenados_na = na_counts.sort_values(ascending=False)\n",
    "plt.bar(ordenados_na.index, ordenados_na)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos histograma de la variable objetivo, para ver su distribución\n",
    "plt.hist(y_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos estadísticas de la variable respuesta (SalePrice)\n",
    "y_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los valores de PoolQC respecto al precio de la casa\n",
    "# plot = House_prices.plot.scatter(x='SalePrice', y=ordenados_na.index[0])\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "ax[0,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[0]])\n",
    "ax[0,0].set_xlabel('SalePrice')\n",
    "ax[0,0].set_ylabel(ordenados_na.index[0])\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[1]])\n",
    "ax[0,1].set_xlabel('SalePrice')\n",
    "ax[0,1].set_ylabel(ordenados_na.index[1])\n",
    "ax[0,1].grid()\n",
    "\n",
    "ax[1,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[2]])\n",
    "ax[1,0].set_xlabel('SalePrice')\n",
    "ax[1,0].set_ylabel(ordenados_na.index[2])\n",
    "ax[1,0].grid()\n",
    "\n",
    "ax[1,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[3]])\n",
    "ax[1,1].set_xlabel('SalePrice')\n",
    "ax[1,1].set_ylabel(ordenados_na.index[3])\n",
    "ax[1,1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en las gráficas comparativas, vemos que los valores que no son NA de los atributos categóricos se mantienen cerca de la media de la variable respuesta con lo cual no aportan información relevante. Es por esto por lo que decidimos directamente eliminar estos atributos categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los 4 atributos que más NA contienen \n",
    "categorical_atr.remove('PoolQC')\n",
    "categorical_atr.remove('MiscFeature')\n",
    "categorical_atr.remove('Alley')\n",
    "categorical_atr.remove('Fence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Para tratar los diferentes atributos lo primero es distinguir entre las variables cuantitativas \n",
    "# (numéricas) y cualitativas (categóricas)\n",
    "\n",
    "# Ya que el tipo de dato 'object' engloba a las variables categóricas y a las variables de tipo\n",
    "# string, se puede usar dicho tipo de dato para distinguir las variables en numéricas y categóricas\n",
    "numerical_atr = [col for col in House_prices.columns if House_prices.dtypes[col] != 'object']\n",
    "categorical_atr = [col for col in House_prices.columns if House_prices.dtypes[col] == 'object']\n",
    "\n",
    "# Eliminamos las variables 'SalePrice' (variable objetico) e 'Id' (no aporta información) de la\n",
    "# lista de variables numéricas\n",
    "numerical_atr.remove('SalePrice')\n",
    "numerical_atr.remove('Id')\n",
    "\n",
    "print(\"Hay \", len(numerical_atr), \" datos numéricos: \",numerical_atr)\n",
    "print(\"Hay \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de nuestro dataset\n",
    "Tras haber realizado un primer análisis inicial, vamos a construir un primer dataset de partida, que será el que usaremos para realizar el análisis exploratorio de datos y la construcción de los modelos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro dataset \n",
    "numerical_data = House_prices[numerical_atr]\n",
    "categorical_data = House_prices[categorical_atr]\n",
    "\n",
    "# Concatenamos las variables numéricas y las variables categóricas para obtener el dataset final\n",
    "X_data = pd.concat([numerical_data, categorical_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el conjunto de datos en tres subconjuntos: entrenamiento, validación y test\n",
    "# En una proprorción 50% - 20% - 30% respectivamente\n",
    "\n",
    "train_size = 0.5\n",
    "val_size = 0.2\n",
    "test_size = 0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(X_data, y_data, test_size=(1.0-train_size), random_state=41)\n",
    "X_val, X_tes, y_val, y_test = train_test_split(X_aux, y_aux, test_size=test_size/(val_size+test_size), random_state=41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las variables numéricas para que todas tengan media 0 y desviación típica 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_atr])\n",
    "X_train[numerical_atr] = scaler.transform(X_train[numerical_atr])\n",
    "X_val[numerical_atr] = scaler.transform(X_val[numerical_atr])\n",
    "X_tes[numerical_atr] = scaler.transform(X_tes[numerical_atr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las variables categoricas\n",
    "X_train[categorical_atr].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de correlación de Pearson entre los atributos\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "\n",
    "sns.heatmap(House_prices.corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap='viridis', linecolor='white', annot=True)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSAS QUE HAY QUE HACER:\n",
    "\n",
    "**Preprocesado de datos**\n",
    "- Eliminar columnas con muchos valores perdidos\n",
    "- One-hot encoding / Label encoding (categorical variables)\n",
    "- Normalizar datos: StandardScaler, MinMaxScaler, RobustScaler \n",
    "- Eliminar outliers: IQR, Z-score, etc.  (?)\n",
    "- Discretizado de variables continuas (?): Binning, etc.\n",
    "\n",
    "\n",
    "**Ingeneering features**\n",
    "- Reducción de dimensionalidad: *PCA*, LDA, etc.\n",
    "- Selección de variables - Filter Methods: Correlation, Chi2, ANOVA, etc.\n",
    "- Multicolinealidad: VIF, etc. \n",
    "- Crear nuevas variables.\n",
    "\n",
    "**Posibles Modelos**\n",
    "- Regresión lineal\n",
    "- Regresión polinómica\n",
    "- Regresión Ridge\n",
    "- Regresión Lasso\n",
    "- Regresión ElasticNet\n",
    "- SVR\n",
    "- Random Forest\n",
    "- **Gradient Boosting**\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "**Evaluación de modelos**\n",
    "- MSE\n",
    "- k-fold cross validation\n",
    "- Recall, Precision, F1-score, etc. (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
