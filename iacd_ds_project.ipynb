{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - EDA and models comparison\n",
    "\n",
    "*Autores: David Tejero Ruiz & Miguel Gil Castilla*\n",
    "\n",
    "El [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) ha sido obtenido de la página web de Kaggle en el apartado de competiciones. Este ha sido escogido debido a que está recomendado como un dataset de \"juguete\" con el que continuar el aprendizaje en python y ciencia del dato a un nivel más o menos básico. Nuestra idea es realizar un exploratory data análisis aplicando técnicas vistas en clase y ampliando estas con algunas ideas captadas de la red, tras esto trataremos nuestro dataset ya modificado para crear un modelo de regresión con el que predecir la variable respuesta *PriceSale*.\n",
    "\n",
    "Una particularidad de este dataset es su elevado número de atributos (79), lo que lo hace muy adecuado para poner en práctica algunas técnicas vistas de ingeniería de características. Así, podemos extraer información relevante de ellas para realizar una predicción más o menos precisa del precio de venta de las diferentes viviendas.\n",
    "\n",
    "\n",
    "Dado que en el conjunto test.csv no se proporciona el valor de la variable respuesta (era objetivo obtenerlo para la competición de Kaggle al que pertenece), nos hemos centrado en el conjunto *train.csv*, que hemos renombrado al archivo *data.csv* que se encuentra en este directorio, y será el conjunto de datos que analicemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el conjunto de datos\n",
    "import pandas as pd\n",
    "House_prices = pd.read_csv('data.csv', keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "# Para evitar avisos innecesarios (FutureWarnings):\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que Pandas por defecto, trata los valores NA (Not Available) como NaN (Not a Number). Si se analiza la descripción de los datos, NA es un valor que puede tomar el atributo, por lo que no se trata de un valor perdido como tal, sino que  indica que no se puede calcular el valor del atributo, porque la vivienda no dispone de el item que se está evaluando. Por ejemplo, en el caso del atributo de calidad de la piscina, NA indica que la vivienda no dispone de piscina, lo cuál es información relevante, y no un valor perdido que tendríamos que imputar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la cabecera de los datos\n",
    "House_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el dataset está compuesto por 81 columnas, de las cuales 79 serán las características que usaremos en la predicción, y las dos restantes sol el id (que no aporta información), y la variable que queremos predecir (precio de venta de la casa).\n",
    "\n",
    "Además notamos que existe una riqueza de características alta, encontrando tanto valores numéricos como categóricos, cosa que analizaremos a continuación en el análisis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a definir un problema de regresión sobre el precio de las casas, por lo que la variable \n",
    "# objetivo del dataset será 'SalePrice' (última columna de nuestro dataset como es habitual)\n",
    "y_data = House_prices['SalePrice']\n",
    "\n",
    "# Para tratar los diferentes atributos lo primero es distinguir entre las variables cuantitativas \n",
    "# (numéricas) y cualitativas (categóricas)\n",
    "\n",
    "# Ya que el tipo de dato 'object' engloba a las variables categóricas y a las variables de tipo\n",
    "# string, se puede usar dicho tipo de dato para distinguir las variables en numéricas y categóricas\n",
    "numerical_atr = [col for col in House_prices.columns if House_prices.dtypes[col] != 'object']\n",
    "categorical_atr = [col for col in House_prices.columns if House_prices.dtypes[col] == 'object']\n",
    "\n",
    "# Eliminamos las variables 'SalePrice' (variable objetico) e 'Id' (no aporta información) de la\n",
    "# lista de variables numéricas\n",
    "numerical_atr.remove('SalePrice')\n",
    "numerical_atr.remove('Id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores perdidos sí se encuentran representados por NaNs en nuestro pandas dataframe, que serán aquellos para los que falte un valor para la característica deseada.\n",
    "\n",
    "Estos valores sí serían susceptibles de realizar una imputación con algunos de los métodos vistos (media, mediana, modelo predictivo, etc.) Sin embargo, nuestro dataset no contiene ninguno de estos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos si existen algun valor nulo\n",
    "print(\"Hay \", House_prices.isnull().sum().sum(), \" valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hay \", len(numerical_atr), \" datos numéricos: \",numerical_atr)\n",
    "print(\"Hay \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, como comentamos al inicio, el dataset sí que contiene valores NA, que realmente podrían aportar información al decir que no se tiene el objeto que tratamos de evaluar en el atributo. Vamos a ver qué atributos contienen valores NA, y cuántos de estos valores NA contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a ver que atributos contienen valores NA\n",
    "na_counts = House_prices.apply(lambda x: x.value_counts().get('NA', 0))\n",
    "na_counts = na_counts[na_counts > 0]\n",
    "print(\"Atributos que contienen NA: \\n{}\".format(na_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos a continuación una gráfica la información anterior\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ordenados_na = na_counts.sort_values(ascending=False)\n",
    "plt.bar(ordenados_na.index, ordenados_na)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitnamos histograma de la variable objetivo\n",
    "plt.hist(y_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos estadísticas de la variable respuesta (SalePrice)\n",
    "y_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los valores de PoolQC respecto al precio de la casa\n",
    "# plot = House_prices.plot.scatter(x='SalePrice', y=ordenados_na.index[0])\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "ax[0,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[0]])\n",
    "ax[0,0].set_xlabel('SalePrice')\n",
    "ax[0,0].set_ylabel(ordenados_na.index[0])\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[1]])\n",
    "ax[0,1].set_xlabel('SalePrice')\n",
    "ax[0,1].set_ylabel(ordenados_na.index[1])\n",
    "ax[0,1].grid()\n",
    "\n",
    "ax[1,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[2]])\n",
    "ax[1,0].set_xlabel('SalePrice')\n",
    "ax[1,0].set_ylabel(ordenados_na.index[2])\n",
    "ax[1,0].grid()\n",
    "\n",
    "ax[1,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[3]])\n",
    "ax[1,1].set_xlabel('SalePrice')\n",
    "ax[1,1].set_ylabel(ordenados_na.index[3])\n",
    "ax[1,1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en las gráficas comparativas, vemos que los valores que no son NA de los atributos categóricos se mantienen cerca de la media de la variable respuesta con lo cual no aportan información relevante. Es por esto por lo que decidimos directamente eliminar estos atributos categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los 4 atributos que más NA contienen \n",
    "House_prices = House_prices.drop(ordenados_na.index[:4], axis=1)\n",
    "# Eliminamos también los Id que no nos aportarán información de valor predicivo\n",
    "House_prices = House_prices.drop('Id', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de correlación de Pearson entre los atributos\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "\n",
    "sns.heatmap(House_prices.corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap='viridis', linecolor='white', annot=True)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que hemos acabado nuestra fase inicial de selección de características, vamos a construir\n",
    "# nuestro dataset final\n",
    "\n",
    "# Dado que las variables categóricas son tratadas de forma diferente a las variables numéricas,\n",
    "# ya que hay que transformarlas en vaiables numéricas, utilizamos dos dataframes diferentes para\n",
    "# almacenar las variables numéricas y las variables categóricas\n",
    "numerical_data = House_prices[numerical_atr]\n",
    "\n",
    "# Para tratar las variables categóricas vamos a usar el método de 'one-hot encoding' que consiste\n",
    "# en crear una nueva variable binaria por cada valor que pueda tomar la variable categórica.\n",
    "# Para ello, usamos la función 'get_dummies' de pandas que efectúa dicho 'one-hot encoding' de\n",
    "# forma automática\n",
    "categorical_data = pd.get_dummies(House_prices[categorical_atr])\n",
    "\n",
    "# Concatenamos las variables numéricas y las variables categóricas para obtener el dataset final\n",
    "X_data = pd.concat([numerical_data, categorical_data], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSAS QUE HAY QUE HACER:\n",
    "\n",
    "**Preprocesado de datos**\n",
    "- Eliminar columnas con muchos valores perdidos\n",
    "- One-hot encoding / Label encoding (categorical variables)\n",
    "- Normalizar datos: StandardScaler, MinMaxScaler, RobustScaler \n",
    "- Eliminar outliers: IQR, Z-score, etc.  (?)\n",
    "- Discretizado de variables continuas (?): Binning, etc.\n",
    "\n",
    "\n",
    "**Ingeneering features**\n",
    "- Reducción de dimensionalidad: *PCA*, LDA, etc.\n",
    "- Selección de variables - Filter Methods: Correlation, Chi2, ANOVA, etc.\n",
    "- Multicolinealidad: VIF, etc. \n",
    "- Crear nuevas variables.\n",
    "\n",
    "**Posibles Modelos**\n",
    "- Regresión lineal\n",
    "- Regresión polinómica\n",
    "- Regresión Ridge\n",
    "- Regresión Lasso\n",
    "- Regresión ElasticNet\n",
    "- SVR\n",
    "- Random Forest\n",
    "- **Gradient Boosting**\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "**Evaluación de modelos**\n",
    "- MSE\n",
    "- k-fold cross validation\n",
    "- Recall, Precision, F1-score, etc. (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
