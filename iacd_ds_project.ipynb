{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - EDA and models comparison\n",
    "\n",
    "*Autores: David Tejero Ruiz & Miguel Gil Castilla*\n",
    "\n",
    "El [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) ha sido obtenido de la página web de Kaggle en el apartado de competiciones. Este ha sido escogido debido a que está recomendado como un dataset de \"juguete\" con el que continuar el aprendizaje en python y ciencia del dato a un nivel más o menos básico. Nuestra idea es realizar un exploratory data análisis aplicando técnicas vistas en clase y ampliando estas con algunas ideas captadas de la red, tras esto trataremos nuestro dataset ya modificado para crear un modelo de regresión con el que predecir la variable respuesta *PriceSale*.\n",
    "\n",
    "Una particularidad de este dataset es su elevado número de atributos (79), lo que lo hace muy adecuado para poner en práctica algunas técnicas vistas de ingeniería de características. Así, podemos extraer información relevante de ellas para realizar una predicción más o menos precisa del precio de venta de las diferentes viviendas.\n",
    "\n",
    "\n",
    "Dado que en el conjunto test.csv no se proporciona el valor de la variable respuesta (era objetivo obtenerlo para la competición de Kaggle al que pertenece), nos hemos centrado en el conjunto *train.csv*, que hemos renombrado al archivo *data.csv* que se encuentra en este directorio, y será el conjunto de datos que analicemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Importamos el conjunto de datos\n",
    "import pandas as pd\n",
    "House_prices = pd.read_csv('data.csv')\n",
    "\n",
    "# Para evitar avisos innecesarios (FutureWarnings):\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que Pandas por defecto, trata los valores NA (Not Available) como NaN (Not a Number). Si se analiza la descripción de los datos, NA es un valor que pueden tomar algunos atributos categóricos (no todos), por lo que no se trata de un valor perdido como tal, sino que  indica que no se puede calcular el valor del atributo, porque la vivienda no dispone de el item que se está evaluando. Por ejemplo, en el caso del atributo de calidad de la piscina, NA indica que la vivienda no dispone de piscina, lo cuál es información relevante, y no un valor perdido que tendríamos que imputar.\n",
    "\n",
    "Hemos analizado la descripción de los diferentes atributos categóricos y aquellos en los que un NA podría aportar información relevante, son:\n",
    "\n",
    "- Alley\n",
    "\n",
    "- BsmtQual\n",
    "\n",
    "- BsmtCond\n",
    "\n",
    "- BsmtExposure\n",
    "\n",
    "- BsmtFinType1\n",
    "\n",
    "- BsmtFinType2\n",
    "\n",
    "- FireplaceQu\n",
    "\n",
    "- GarageType\n",
    "\n",
    "- GarageYrBlt\n",
    "\n",
    "- GarageFinish\n",
    "\n",
    "- GarageQual\n",
    "\n",
    "- GarageCond\n",
    "\n",
    "- PoolQC\n",
    "\n",
    "- Fence\n",
    "\n",
    "- MiscFeature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_attributes = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                 'PoolQC', 'Fence', 'MiscFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la cabecera de los datos\n",
    "House_prices.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el dataset está compuesto por 81 columnas, de las cuales 79 serán las características que usaremos en la predicción, y las dos restantes sol el id (que no aporta información), y la variable que queremos predecir (precio de venta de la casa).\n",
    "\n",
    "Además notamos que existe una riqueza de características alta, encontrando tanto valores numéricos como categóricos, cosa que analizaremos a continuación en el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a definir un problema de regresión sobre el precio de las casas, por lo que la variable \n",
    "# objetivo del dataset será 'SalePrice' (última columna de nuestro dataset como es habitual)\n",
    "y_data = House_prices['SalePrice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de missing values y non-available values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentamos al inicio, el dataset contiene valores NA, y en algunos casos realmente podrían aportar información útil al decir que no se tiene el objeto que tratamos de evaluar en el atributo. Vamos a ver qué atributos contienen valores NA, y cuántos de estos valores NA contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a contar las apariciones de NaNs en nuestro dataset al completo\n",
    "print(\"Hay \", House_prices.isnull().sum().sum(), \" valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el conjunto de datos en atributos con posibles valores NA y \n",
    "# atributos con posibles missing values\n",
    "House_prices_na = House_prices[na_attributes]\n",
    "House_prices_mv = House_prices.drop(na_attributes, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuales de los atributos en los que un NaN significa Missing Value \n",
    "# tienen realmente valores NaN\n",
    "mv_counts = House_prices_mv.isnull().sum()\n",
    "mv_counts = mv_counts[mv_counts > 0]\n",
    "print(\"Atributos que contienen missing values: \\n{}\".format(mv_counts))\n",
    "print(\"Hay \", len(mv_counts), \" atributos que contienen missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De entre aquellos en los que NA no es un valor posible del atributo,\n",
    "# y las apariciones de NaN son datos pérdidos, vamos a ver cuáles son \n",
    "# numéricos y cuáles no\n",
    "House_prices_mv_numerics = House_prices_mv[mv_counts.keys()].select_dtypes(include=[np.number])\n",
    "House_prices_mv_categorics = House_prices_mv[mv_counts.keys()].select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen missing values: \\n{}\".format(House_prices_mv_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen missing values: \\n{}\".format(House_prices_mv_categorics.keys().to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos numéricos que contienen missing values los vamos a rellenar con la media\n",
    "# de los valores de la columna\n",
    "House_prices_mv_numerics = House_prices_mv_numerics.fillna(House_prices_mv_numerics.mean())\n",
    "\n",
    "# Los atributos categóricos que contienen missing values los vamos a rellenar con el valor\n",
    "# más frecuente de la columna\n",
    "House_prices_mv_categorics = House_prices_mv_categorics.fillna(House_prices_mv_categorics.mode().iloc[0])\n",
    "\n",
    "# Unimos los dos conjuntos de datos\n",
    "House_prices_mv_fixed = pd.concat([House_prices_mv_numerics, House_prices_mv_categorics], axis=1)\n",
    "\n",
    "# Guardamos los datos tratados\n",
    "for key in mv_counts.keys():\n",
    "    House_prices[key] = House_prices_mv_fixed[key]\n",
    "\n",
    "# Vamos a comprobar que ya no hay valores nulos en los atributos que hemos tratado\n",
    "if (House_prices[mv_counts.keys()].isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Available values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que atributos contienen valores NA entre los que NA aporta información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuales de los atributos en los que un NaN significa Missing Value \n",
    "# tienen realmente valoress NaN\n",
    "na_counts = House_prices_na.isnull().sum()\n",
    "na_counts = na_counts[na_counts > 0]\n",
    "print(\"Atributos que contienen Non Available values: \\n{}\".format(na_counts))\n",
    "print(\"Hay \", len(na_counts), \" atributos que contienen Non Available values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na.select_dtypes(include=[np.number])\n",
    "House_prices_na_categorics = House_prices_na.select_dtypes(exclude=[np.number])\n",
    "print(\"Atributos numericos que contienen Non Available values: \\n{}\".format(House_prices_na_numerics.keys().to_list()))\n",
    "print(\"Atributos categóricos que contienen Non Available values: \\n{}\".format(House_prices_na_categorics.keys().to_list()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar tratando el caso especial del único atributo numérico que tiene un NA como valor informativo, hablamos del **GarageYrBlt**. Este atributo codifica el año en el que se construyó el garaje de la casa, y un valor NA indica que la casa no tiene garaje. La información de \"no tenemos garaje\" puede ser relevante, por lo que tenemos que tratar este caso cuidosamente.\n",
    "\n",
    "Sin embargo, parte de los atributos que analizaremos un poco más adelante: GarageType, GarageFinish, GarageCond y Garageal, son atributos categóricos cuyo valor NA indica igualmente \"no tenemos garaje\", por lo que esa información en este atributo en particular es redundante.\n",
    "\n",
    "Por tanto, hemos llegado a la decisión de sustituir los valores NA de GarageYrBlt por la media de los valores no NA de este atributo. De esta forma, cuando posteriormente le apliquemos una normalización estándar al atributo y le restemos la media, obtendremos un valor nulo, y por tanto no afectará a la predicción dado a que así indicaremos que no es relevante ese valor en este atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "House_prices_na_numerics = House_prices_na_numerics.fillna(House_prices_na_numerics.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el resto de valores NA, los NaNs indican un posible valor categorico, por lo que vamos a sustituirlos por la cadena \"None\", para que se traten como un posible valor categórico más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos los NaNs con la cadena 'None\n",
    "House_prices_na_categorics = House_prices_na_categorics.fillna('None')\n",
    "\n",
    "# Unimos los dos conjuntos de datos tratados\n",
    "House_prices_na_fixed = pd.concat([House_prices_na_numerics, House_prices_na_categorics], axis=1)\n",
    "\n",
    "# Guardamos los datos tratados\n",
    "for key in na_counts.keys():\n",
    "    House_prices[key] = House_prices_na_fixed[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a comprobar que ya no hay valores nulos en nuestro conjunto de datos\n",
    "if (House_prices.isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase inicial de ingeniería de características\n",
    "Vamos a analizar estas sustituticiones con los valores 'None' que repercusión tiene sobre el conjunto de datos. Se mostrará una gráfica con la cantidad de estos valores para valorar si es necesario seguir con la exploración o simplemente eliminar las estancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a graficar los valores NA que susituimos por 'None'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ordenados_na = na_counts.sort_values(ascending=False)\n",
    "plt.bar(ordenados_na.index, ordenados_na)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Número de valores NA')\n",
    "plt.xlabel('Atributos')\n",
    "plt.title('Valores NA sustituidos por None')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print de los 4 mayores:\n",
    "print(\"Los 4 mayores valores NA sustituidos por None son:\")\n",
    "print(ordenados_na.head(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que existe una cantidad exuberante de datos marcados con None, superando el 50% de las instancias, para 4 de los atributos, concretamente:\n",
    " - PoolQC: 1453\n",
    " - MiscFeature: 1406 \n",
    " - Alley: 1369\n",
    " - Fence: 1179\n",
    " \n",
    "Debemos de considerar que el número total de instancias en el dataset es de **1460**.\n",
    "\n",
    "Para ello dibujaremos el par atributo-variable respuesta para ver la relación entre estos dos, concretamente en los valores que no son None. Con esta comparación veremos si es relevante la información que nos aporta o no el atributo, decidiendo si eliminarlo o no.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos histograma de la variable objetivo, para ver su distribución\n",
    "plt.hist(y_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da la intuición de dónde se sitúa la media de la variable respuesta y somo es su distribución, con ella podremos decidir si eliminar o no algunso atributos. Nuestra idea es que si los valores que no son 'None' son pocos y se sitúan cercanos a la donde de encuentran la mayoría de instancias (cercanos a la media de la variable respuesta), probablemente estos atributos no aportarán información relevante para la regresión, comparado con la información de otros atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los valores  de las 4 variables con mayor número de valores NA\n",
    "# en aquellos que son diferentes de None.\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "ax[0,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[0]])\n",
    "ax[0,0].set_xlabel('SalePrice')\n",
    "ax[0,0].set_ylabel(ordenados_na.index[0])\n",
    "ax[0,0].grid()\n",
    "\n",
    "ax[0,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[1]])\n",
    "ax[0,1].set_xlabel('SalePrice')\n",
    "ax[0,1].set_ylabel(ordenados_na.index[1])\n",
    "ax[0,1].grid()\n",
    "\n",
    "ax[1,0].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[2]])\n",
    "ax[1,0].set_xlabel('SalePrice')\n",
    "ax[1,0].set_ylabel(ordenados_na.index[2])\n",
    "ax[1,0].grid()\n",
    "\n",
    "ax[1,1].scatter(House_prices['SalePrice'], House_prices[ordenados_na.index[3]])\n",
    "ax[1,1].set_xlabel('SalePrice')\n",
    "ax[1,1].set_ylabel(ordenados_na.index[3])\n",
    "ax[1,1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en las gráficas comparativas, se observa que los atributos toman 'None' a lo largo de todo el rango de valores de SalePrice. Vemos que los valores que no son None de los atributos categóricos son pocos y se encuentran además en la zona donde tenemos un alto número de instancias, con lo cual, no aportan información relevante en este problema de regresión. Es por esto por lo que decidimos directamente eliminar estos atributos categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los 4 atributos que más NA contienen \n",
    "House_prices = House_prices.drop(ordenados_na.head(4).index, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de nuestro dataset\n",
    "Tras haber realizado un primer análisis inicial, vamos a construir el dataset de partida, que será el que usaremos para realizar el análisis exploratorio de datos y la construcción de los modelos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el dataset de datos de entrada (X_data) como el dataset sin la variable objetivo\n",
    "# y sin la variable 'Id' (que no aporta información)\n",
    "X_data = House_prices.drop(['SalePrice', 'Id'], axis=1)\n",
    "\n",
    "# Para tratar los diferentes atributos lo primero es distinguir entre las variables cuantitativas \n",
    "# (numéricas) y cualitativas (categóricas)\n",
    "# Ya que el tipo de dato 'object' engloba a las variables categóricas y a las variables de tipo\n",
    "# # string, se puede usar dicho tipo de dato para distinguir las variables en numéricas y categóricas\n",
    "numerical_atr = [col for col in X_data.columns if X_data.dtypes[col] != 'object']\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "\n",
    "print(\"Hay \", len(numerical_atr), \" datos numéricos: \",numerical_atr)\n",
    "print(\"Hay \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las variables categoricas\n",
    "X_data[categorical_atr].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como bien sabemos, estos atributos categóricos debemos transformarlos a valores numéricos para poder trabajar con ellos. Existen diferentes técnicas para realizar esto, y para hacerlo de forma consistente, debemos analizar la naturaleza de los diferentes atributos categóricos.\n",
    "\n",
    "Notamos que existen una serie de variables categóricas a cuyos atributos podemos asignarles un valor numérico según un orden. Estos casos son cuando hablamos de calidades (a mayor calidad vamos a asignar un valor superior) o cuando cuantificamos con palabras una cantidad (por ejemplo, el LandSlope caracteriza la pendiente del terreno, y podemos asignarle un valor numérico mayor a una mayor pendiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basándonos en el archivo data_description.txt codificamos las variables categóricas con información cuantitativa\n",
    "X_data['LotShape'] = X_data['LotShape'].map({'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0})\n",
    "X_data['LandSlope'] = X_data['LandSlope'].map({'Gtl': 0, 'Mod': 1, 'Sev': 2})\n",
    "X_data['ExterQual'] = X_data['ExterQual'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['ExterCond'] = X_data['ExterCond'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['BsmtQual'] = X_data['BsmtQual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['BsmtCond'] = X_data['BsmtCond'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['BsmtExposure'] = X_data['BsmtExposure'].map({'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0})\n",
    "X_data['BsmtFinType1'] = X_data['BsmtFinType1'].map({'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['BsmtFinType2'] = X_data['BsmtFinType2'].map({'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['HeatingQC'] = X_data['HeatingQC'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['KitchenQual'] = X_data['KitchenQual'].map({'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0})\n",
    "X_data['Functional'] = X_data['Functional'].map({'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0})\n",
    "X_data['FireplaceQu'] = X_data['FireplaceQu'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['GarageFinish'] = X_data['GarageFinish'].map({'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0})\n",
    "X_data['GarageQual'] = X_data['GarageQual'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['GarageCond'] = X_data['GarageCond'].map({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0})\n",
    "X_data['PavedDrive'] = X_data['PavedDrive'].map({'Y': 2, 'P': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas dos características podemos tratarlas como binarias al tener solo dos posibles valores\n",
    "X_data['CentralAir'] = X_data['CentralAir'].map({'Y': 1, 'N': 0})\n",
    "X_data['Street'] = X_data['Street'].map({'Pave': 1, 'Grvl': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las que nos quedan, son variables categóricas que no podemos ordenar, y por tanto, debemos aplicar una codificación one-hot. Para ello, vamos a usar la función OneHotEncoder de sklearn, que nos permite realizar esta codificación de forma sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculamos los valores que siguen siendo categóricos\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "print(\"Quedan \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Usamos OneHotEncoder para codificar las variables categóricas\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_data_encoded = pd.DataFrame(encoder.fit_transform(X_data[categorical_atr]))\n",
    "X_data_encoded.columns = encoder.get_feature_names(categorical_atr)\n",
    "\n",
    "# Borramos las variables categóricas originales y añadimos las nuevas codificadas\n",
    "X_data = X_data.drop(categorical_atr ,axis=1)\n",
    "X_data = pd.concat([X_data, X_data_encoded], axis=1)\n",
    "\n",
    "# Mostramos los datos de entrada\n",
    "X_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencionar aunque 214 parezcan muchos atributos, gracias al label encoding manual que hemos realizado arriba usando información de la descripción de los datos, nos hemos ahorrado hacer one-hot-encoding de todos los atributos categóricos, lo cual supondría obtener bastantes más atributos, aumentando consideralmente la complejidad del problema.\n",
    "\n",
    "Notar que hemos optado por usar one-hot-encoding para las variables en las que no podemos definir un orden, y no otro tipo de codificaciones, para no introducir un orden artificial que no existiría en la realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos que no hay valores nulos en el conjunto final de datos\n",
    "if (X_data.isnull().sum().sum() == 0):\n",
    "    print(\"No hay valores nulos en estos conjuntos\")\n",
    "\n",
    "# Probamos que no quedan variables categóricas\n",
    "categorical_atr = [col for col in X_data.columns if X_data.dtypes[col] == 'object']\n",
    "print(\"Quedan \", len(categorical_atr), \" datos categóricos: \",categorical_atr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingeniería de características avanzada"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multicolinealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenermos la matriz de correlación de los datos de entrada\n",
    "correlation_matrix = X_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de correlación de Pearson entre los atributos\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "sns.heatmap(correlation_matrix, cmap='viridis',vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a identificar pares de atributos con correlación mayor a 0.7\n",
    "# y vamos a eliminar uno de los dos atributos de cada par\n",
    "# para evitar la multicolinealidad\n",
    "\n",
    "# Mantenemos una lista de atributos eliminados para no eliminarlos dos veces, y simplifcamos\n",
    "\n",
    "# La matriz es simétrica, por lo que definimos los bucles para recorrer solo la mitad superior\n",
    "# de la matriz\n",
    "\n",
    "# Tenemos en cuenta tanto correlación positiva como negativa, por lo que usamos el valor absoluto\n",
    "\n",
    "umbral = 0.7\n",
    "atributos_eliminados = []\n",
    "for i in range(len(correlation_matrix.columns)): \n",
    "    for j in range(i+1, len(correlation_matrix.columns)): \n",
    "        if (np.abs(correlation_matrix.iloc[i, j]) >= umbral):\n",
    "            if (correlation_matrix.columns[j] not in atributos_eliminados):\n",
    "                atributo = correlation_matrix.columns[j]\n",
    "                atributos_eliminados.append(atributo)\n",
    "                print(\"Eliminamos el atributo \", atributo, \" por tener una correlación mayor a \", umbral, \" con el atributo \", correlation_matrix.columns[i])\n",
    "\n",
    "print(\"Se van a eliminar \", len(atributos_eliminados), \" atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los atributos de la lista de atributos a eliminar\n",
    "X_data = X_data.drop(atributos_eliminados, axis=1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metodos de filtrado para selección de características\n",
    "A continuación, vamos a estudiar la correlación de los atributos con la variable respuesta, para ver si podemos eliminar algunos atributos que no aporten información relevante para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.DataFrame(pd.concat([X_data, y_data], axis=1)).corr()\n",
    "correlation_matrix = correlation_matrix['SalePrice']\n",
    "\n",
    "# Ploteamos la matriz de correlación de los datos de entrada\n",
    "plt.figure(figsize=(15, 0.5))\n",
    "plt.title('Coeficiente de Correlación de Pearson entre los atributos', y=1.05, size=15)\n",
    "sns.heatmap(correlation_matrix.to_frame().T, cmap='viridis', vmax=1, vmin=-1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a eliminar los atributos que tengan una correlación pequeña con la variable objetivo\n",
    "umbral = 0.025\n",
    "atributos_a_eliminar = []\n",
    "for i in range(len(correlation_matrix)):\n",
    "    if (np.abs(correlation_matrix[i]) < umbral):\n",
    "        atributo = correlation_matrix.index[i]\n",
    "        atributos_a_eliminar.append(atributo)\n",
    "        print(\"Eliminamos el atributo \", atributo, \" por tener una correlación menor a \", umbral, \" con la variable objetivo\")\n",
    "\n",
    "print(\"Se van a eliminar \", len(atributos_a_eliminar), \" atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los atributos de la lista de atributos a eliminar\n",
    "X_data = X_data.drop(atributos_a_eliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el conjunto de datos en tres subconjuntos: entrenamiento, validación y test\n",
    "# En una proprorción 50% - 20% - 30% respectivamente\n",
    "\n",
    "train_size = 0.89\n",
    "val_size = 0.01\n",
    "test_size = 0.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(X_data, y_data, test_size=(1.0-train_size), random_state=41)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=test_size/(val_size+test_size), random_state=41)\n",
    "\n",
    "# Mostramos los tamaños de los conjuntos de datos\n",
    "print(\"Tamaño del conjunto de entrenamiento: \", len(X_train))\n",
    "print(\"Tamaño del conjunto de validación: \", len(X_val))\n",
    "print(\"Tamaño del conjunto de test: \", len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos las variables numéricas para que todas tengan media 0 y desviación típica 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_data.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_data.columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_data.columns)\n",
    "\n",
    "# Mostramos los datos de entrada\n",
    "X_train.head()\n",
    "\n",
    "X_data = pd.concat([X_train, X_val, X_test])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenamos gradient boosting con los datos de entrenamiento\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                             \n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "err = mean_absolute_error(y_test, y_pred)/np.mean(y_test)\n",
    "print ('%.2f'%err)\n",
    "y_pred = gbr.predict(X_train)\n",
    "err = mean_absolute_error(y_train, y_pred)/np.mean(y_train)\n",
    "print ('%.2f'%err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See sales\n",
    "plt.scatter(y_train, y_pred)\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COSAS QUE HAY QUE HACER:\n",
    "\n",
    "**Preprocesado de datos**\n",
    "- Eliminar columnas con muchos valores perdidos\n",
    "- One-hot encoding / Label encoding (categorical variables)\n",
    "- Normalizar datos: StandardScaler, MinMaxScaler, RobustScaler \n",
    "- Eliminar outliers: IQR, Z-score, etc.  (?)\n",
    "- Discretizado de variables continuas (?): Binning, etc.\n",
    "\n",
    "\n",
    "**Ingeneering features**\n",
    "- Reducción de dimensionalidad: *PCA*, LDA, etc.\n",
    "- Selección de variables - Filter Methods: Correlation, Chi2, ANOVA, etc.\n",
    "- Multicolinealidad: VIF, etc. \n",
    "- Crear nuevas variables.\n",
    "\n",
    "**Posibles Modelos**\n",
    "- Regresión lineal\n",
    "- Regresión polinómica\n",
    "- Regresión Ridge\n",
    "- Regresión Lasso\n",
    "- Regresión ElasticNet\n",
    "- SVR\n",
    "- Random Forest\n",
    "- **Gradient Boosting**\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "**Evaluación de modelos**\n",
    "- MSE\n",
    "- k-fold cross validation\n",
    "- Recall, Precision, F1-score, etc. (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
